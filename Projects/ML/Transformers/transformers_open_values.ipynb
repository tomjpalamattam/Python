{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e4327baf-608a-4c67-ac98-ea4cd1e9f677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "#from transformers import GPT2Tokenizer\n",
    "import string\n",
    "from torch.nn import functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1a390873-fd3f-439b-8024-a9924f6f4c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7159057144f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1877fe4c-d40c-4f8e-99a8-08dd36c31900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5993</th>\n",
       "      <td>2000-12-25</td>\n",
       "      <td>274.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5992</th>\n",
       "      <td>2000-12-26</td>\n",
       "      <td>273.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5991</th>\n",
       "      <td>2000-12-27</td>\n",
       "      <td>274.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5990</th>\n",
       "      <td>2000-12-28</td>\n",
       "      <td>275.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5989</th>\n",
       "      <td>2000-12-29</td>\n",
       "      <td>272.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-12-26</td>\n",
       "      <td>2055.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-12-27</td>\n",
       "      <td>2066.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-12-28</td>\n",
       "      <td>2077.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-12-29</td>\n",
       "      <td>2066.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>2064.24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5994 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date     Open\n",
       "5993 2000-12-25   274.00\n",
       "5992 2000-12-26   273.40\n",
       "5991 2000-12-27   274.55\n",
       "5990 2000-12-28   275.25\n",
       "5989 2000-12-29   272.45\n",
       "...         ...      ...\n",
       "4    2023-12-26  2055.73\n",
       "3    2023-12-27  2066.70\n",
       "2    2023-12-28  2077.34\n",
       "1    2023-12-29  2066.82\n",
       "0    2024-01-01  2064.24\n",
       "\n",
       "[5994 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/home/tom/Python/Machine learning/pytorch/lstm/files/XAU_USD Historical Data.csv' , thousands=',')\n",
    "\n",
    "df = df[['Date', 'Open']]\n",
    "# Convert 'Date' to datetime and sort by date\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.sort_values('Date', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "21442480-6139-4030-8fb0-8f1430fa829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = df['Open'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "02cc9426-18d2-4e00-a81e-64b58d817978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5994]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "open_values = torch.tensor(df['Open'].values.astype(np.float32)).view(-1).long()\n",
    "print(open_values.shape, open_values.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e7cd6ed-1a68-4d4d-aa93-7dad0d4a5597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5394])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train and test splits\n",
    "n = int(0.9*len(open_values)) # first 90% will be train, rest val data\n",
    "train_data = open_values[:n]\n",
    "val_data = open_values[n:]\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "61fd0c0b-c6ad-4d4d-9bb7-baf4c0357a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    # Generate a small batch of data with inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix]).to(device)\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix]).to(device)\n",
    "    return x, y\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c37e1b1a-22ad-49eb-9579-b8f801442746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 32])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X , Y = get_batch(train_data)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "77c545c2-3d2c-4ffa-b0c0-8e532efbc8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\"One head of self-attention\"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)   # (B, T, C)\n",
    "        q = self.query(x) # (B, T, C)\n",
    "\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))  # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "\n",
    "        # Perform the weighted aggregation of the values\n",
    "        v = self.value(x)  # (B, T, C)\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multiple heads of self-attention in parallel\"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"A simple linear layer followed by a non-linearity\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "# Super simple bigram model\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)  # Final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B, T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx)  # (B, T, C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # (T, C)\n",
    "        x = tok_emb + pos_emb  # (B, T, C)\n",
    "        x = self.blocks(x)  # (B, T, C)\n",
    "        x = self.ln_f(x)  # (B, T, C)\n",
    "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # Get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # Focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # Becomes (B, C)\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # Sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # Append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c42ed31-067b-4e3d-8871-80004ff35f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913811 M parameters\n",
      "step 0: train loss 8.7800, val loss 8.8093\n",
      "step 100: train loss 6.9001, val loss 9.2589\n",
      "step 200: train loss 5.7120, val loss 9.1746\n",
      "step 300: train loss 4.1133, val loss 9.0375\n",
      "step 400: train loss 2.7689, val loss 9.3730\n",
      "step 500: train loss 1.9449, val loss 9.8738\n",
      "step 600: train loss 1.4415, val loss 10.1541\n",
      "step 700: train loss 1.0684, val loss 10.6645\n",
      "step 800: train loss 0.8284, val loss 10.8275\n",
      "step 900: train loss 0.6476, val loss 11.1216\n",
      "step 1000: train loss 0.5308, val loss 11.4122\n",
      "step 1100: train loss 0.4492, val loss 11.6726\n",
      "step 1200: train loss 0.4087, val loss 11.8729\n",
      "step 1300: train loss 0.3630, val loss 11.9393\n",
      "step 1400: train loss 0.3481, val loss 12.2605\n",
      "step 1500: train loss 0.3149, val loss 12.0779\n",
      "step 1600: train loss 0.2839, val loss 12.4909\n",
      "step 1700: train loss 0.2743, val loss 12.5246\n",
      "step 1800: train loss 0.2632, val loss 12.7763\n",
      "step 1900: train loss 0.2539, val loss 12.7153\n",
      "step 2000: train loss 0.2382, val loss 12.9804\n",
      "step 2100: train loss 0.2299, val loss 13.0541\n",
      "step 2200: train loss 0.2186, val loss 13.0110\n",
      "step 2300: train loss 0.2255, val loss 13.3930\n",
      "step 2400: train loss 0.2026, val loss 13.2499\n",
      "step 2500: train loss 0.1952, val loss 13.4685\n",
      "step 2600: train loss 0.1856, val loss 13.3775\n",
      "step 2700: train loss 0.1701, val loss 13.6016\n",
      "step 2800: train loss 0.1791, val loss 13.5832\n",
      "step 2900: train loss 0.1675, val loss 13.5972\n",
      "step 3000: train loss 0.1655, val loss 13.7950\n",
      "step 3100: train loss 0.1542, val loss 14.0582\n",
      "step 3200: train loss 0.1478, val loss 13.8712\n",
      "step 3300: train loss 0.1456, val loss 14.0797\n",
      "step 3400: train loss 0.1461, val loss 14.2566\n",
      "step 3500: train loss 0.1398, val loss 14.2176\n",
      "step 3600: train loss 0.1360, val loss 14.1662\n",
      "step 3700: train loss 0.1318, val loss 14.2947\n",
      "step 3800: train loss 0.1303, val loss 14.2287\n",
      "step 3900: train loss 0.1265, val loss 14.4691\n",
      "step 4000: train loss 0.1264, val loss 14.2442\n",
      "step 4100: train loss 0.1304, val loss 14.6152\n",
      "step 4200: train loss 0.1257, val loss 14.4668\n",
      "step 4300: train loss 0.1264, val loss 14.5146\n",
      "step 4400: train loss 0.1185, val loss 14.5359\n",
      "step 4500: train loss 0.1198, val loss 14.6118\n",
      "step 4600: train loss 0.1127, val loss 14.6124\n",
      "step 4700: train loss 0.1249, val loss 14.7561\n",
      "step 4800: train loss 0.1185, val loss 14.8977\n",
      "step 4900: train loss 0.1173, val loss 14.7187\n",
      "step 4999: train loss 0.1186, val loss 15.0185\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "924b15af-311f-49ac-bae7-e8cf37b8b166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1383, 1393, 1396, 1380, 1371, 1376, 1385, 1385, 1380, 1384, 1383, 1405, 1411, 1406, 1421, 1414, 1380, 1377, 1372, 1369, 1374, 1381, 1388, 1373, 1359, 1361, 1368, 1370, 1345, 1343, 1334, 1333, 1346, 1312, 1336, 1332, 1340, 1336, 1353, 1348, 1350, 1364, 1363, 1363, 1359, 1362, 1373, 1374, 1383, 1388, 1406, 1399, 1411, 1402, 1408, 1410, 1434, 1435, 1416, 1428, 1431, 1428, 1429, 1412, 1422, 1429, 1396, 1401, 1405, 1426, 1427, 1429, 1439, 1432, 1427, 1422, 1418, 1424, 1432, 1428, 1437, 1456, 1460, 1460, 1473, 1462, 1452, 1459, 1476, 1486, 1497, 1497, 1503, 1507, 1511, 1505, 1506, 1527, 1536, 1567, 1545, 1537, 1516, 1474, 1493, 1506, 1493, 1491, 1486, 1498, 1495, 1512, 1518, 1527, 1524, 1523, 1537, 1539, 1535, 1542, 1535, 1544, 1543, 1545, 1538, 1545, 1530, 1516, 1523, 1530, 1528, 1538, 1540, 1548, 1549, 1522, 1500, 1496, 1501, 1512, 1502, 1486, 1496, 1517, 1530, 1533, 1544, 1555, 1566, 1582, 1588, 1595, 1605, 1589, 1598, 1592, 1599, 1614, 1618, 1613, 1616, 1628, 1620, 1659, 1661, 1648, 1662, 1717, 1743, 1796, 1763, 1746, 1766, 1786, 1789, 1824, 1859, 1897, 1830, 1750, 1772, 1837, 1787, 1835, 1823, 1824, 1877, 1903, 1872, 1816, 1868, 1857, 1813, 1833, 1820, 1788, 1818, 1778, 1803, 1779, 1737, 1657, 1624, 1650, 1606, 1615, 1618, 1660, 1624, 1720, 1736, 351, 351, 357, 356, 356, 358, 362, 363, 368, 369, 369, 365, 370, 368, 347, 347, 349, 351, 351, 352, 356, 361, 358, 362, 366, 363, 358, 361, 366, 361, 362, 361, 364, 372, 369, 374, 376, 371, 375, 373, 376, 375, 382, 380, 379, 375, 374, 373, 376, 376, 382, 386, 385, 389, 384, 381, 383, 385, 385, 383, 371, 373, 376, 375, 370, 373, 374, 375, 372, 372, 373, 380, 384, 384, 388, 388, 383, 385, 383, 384, 377, 380, 382, 380, 383, 385, 388, 394, 394, 399, 390, 398, 394, 394, 396, 391, 391, 396, 1654, 934, 953, 957, 956, 1843, 1846, 1828, 1838, 1840, 1871, 1870, 1855, 1855, 1850, 1844, 1840, 1863, 1860, 1837, 1834, 1792, 1815, 1830, 1837, 1842, 1825, 1825, 1819, 1794, 1776, 1776, 1782, 1809, 1805, 1804, 1770, 1733, 1724, 1738, 1711, 1698, 1702, 1682, 1715, 1726, 1721, 1727, 1730, 1730, 1745, 1735, 1745, 1739, 1727, 1735, 1727, 1732, 1712, 1685, 1708, 1730, 1728, 1728, 1742, 1737, 1756, 1743, 1732, 1744, 1737, 1763, 1779, 1770, 1778, 1794, 1784, 1776, 1780, 1776, 1780, 1772, 1768, 1792, 1778, 1787, 1815, 1833, 1834, 1837, 1815, 1826, 1843, 1866, 1869, 1869, 1876, 1882, 1880, 1899, 1896, 1897, 1903, 1907, 1900, 1908, 1871, 1888, 1898, 1893, 1888, 1898, 1874, 1866, 1858, 1812, 1772, 1764, 1783, 1778, 1779, 1776, 1784, 1794, 1810, 1801, 1798, 1802, 1808, 1811, 1797, 1809, 1815, 1841, 1871, 1886, 1899, 1942, 1959, 1970, 1960, 1980, 1976, 2018, 2040, 2064, 2033, 2027, 1911, 1920, 1953, 1946, 1985, 2001, 1929, 1946, 1938, 1932, 1928, 1953, 1929, 1965, 1967, 1969, 1943, 1931, 1930, 1929, 1932, 1946, 1955, 1941, 1957, 1956, 1959, 1943, 1949, 1912, 1900, 1863, 1868, 1859, 1881, 1897, 1885, 1905, 1902, 1913, 1878, 1887, 1894, 1930, 1922, 1891, 1900, 1907, 1899, 1904, 1908, 1924, 1904, 1902, 1902, 1902, 1908, 1876, 1868, 1878, 1895, 1908, 939, 954, 947, 1645, 1644, 1666, 1566, 1582, 1582, 1588, 1595, 1605, 1589, 1598, 1592, 1599, 1614, 1618, 1613, 1616, 1628, 1620, 1659, 1661, 1648, 1662, 1717, 1743, 1796, 1763, 1746, 1766, 1786, 1789, 1824, 1859, 1897, 1830, 1750, 1772, 1837, 1787, 1835, 1823, 1824, 1877, 1903, 1872, 1816, 1868, 1857, 1813, 1833, 1820, 1788, 1818, 1778, 1803, 1779, 1737, 1657, 1624, 1650, 1606, 1615, 1618, 1660, 1635, 1640, 1642, 1590, 1590, 1639, 1635, 1670, 1694, 1678, 1649, 1634, 1576, 1550, 1512, 1527, 1486, 1470, 1508, 1565, 1618, 1620, 1634, 1617, 1612, 1569, 1590, 1609, 1596, 1662, 1645, 1644, 1675, 1682, 1714, 1729, 1716, 1713, 1691, 1694, 1686, 1713, 1733, 1724, 1716, 1708, 1715, 1685, 1697, 1701, 1705, 1686, 1717, 1703, 1696, 1701, 1715, 1731, 1742, 1733, 1744, 1750, 1725, 1734, 1729, 1712, 1709, 1721, 1736, 1740, 1728, 1699, 1711, 1684, 1700, 1714, 1737, 1728, 1732, 1725, 1727, 1725, 1723, 1749, 1756, 1743, 1732, 1728, 1723, 1734, 1759, 1776, 1780, 1773, 1767, 1784, 1696, 1717, 1711, 1705, 1674, 1685, 1700, 1709, 1699, 1675, 1642, 1659, 1654, 1666, 1651, 1652, 1645, 1661, 1690, 1680, 1661, 1661, 1668, 1676, 1644, 1620, 1628, 1641, 1641, 1659, 1659, 1675, 1654, 1652, 1651, 1643, 1643, 1643, 1638, 1641, 1645, 1657, 1662, 1666, 1662, 1653, 1636, 1641, 1638, 1605, 1589, 1593, 1580, 1124, 1113, 410, 407, 406, 411, 1277, 2027, 1911, 1920, 1953, 1929, 1965, 1967, 1969, 1943, 1931, 1930, 1929, 1932, 1946, 1955, 1941, 1957, 1956, 1959, 1943, 1949, 1912, 1900, 1863, 1868, 1859, 1881, 1897, 1885, 1905, 1902, 1913, 1878, 1887, 1894, 1930, 1922, 1891, 1900, 1907, 1899, 1904, 1908, 1924, 1904, 1902, 1902, 1908, 1876, 1868, 1878, 1895, 1908, 1903, 1949, 1954, 1862, 1877, 1865, 1876, 1888, 1889, 1878, 1871, 1867, 1869, 1836, 1807, 1805, 1811, 1789, 1777, 1815, 1831, 1841, 1837, 1863, 1871, 1839, 1835, 1839, 1827, 1853, 1864, 1885, 1883, 1876, 1859, 1872, 1877, 1885, 1872, 1878, 1893, 1897, 1906, 1942, 1950, 1919, 1912, 1849, 1845, 1856, 1843, 1846, 1828, 1838, 1840, 1871, 1870, 1855, 1855, 1850, 1844, 1840, 1863, 1860, 1837, 1834, 1792, 1815, 1830, 1837, 1842, 1825, 1825, 1819, 1794, 1776, 1776, 1782, 1809, 1805, 1804, 1770, 1733, 1724, 1738, 1711, 1698, 1702, 1682, 1715, 1726, 1721, 1727, 1730, 1730, 1745, 1735, 1745, 1739, 1727, 1735, 1727, 1732, 1712, 1685, 1708, 1730, 1728, 1728, 1742, 1737, 1756, 1743, 1732, 1744, 1737, 1763, 1779, 1770, 1778, 1794, 1784, 1776, 1780, 1776, 1780, 1772, 1768, 1792, 1778, 1787, 1815, 1833, 1834, 1837, 1815, 1826, 1843, 1866, 1869, 1869, 1876, 1882, 1880, 1899, 1896, 1897, 1903, 1907, 1900, 1908, 1871, 1888, 1898, 1893, 1888, 1898, 1874, 1866, 1858, 1812, 1772, 1764, 1783, 1778, 1779, 1775, 1781, 1762, 1722, 1723, 1876, 1868, 1857, 1813, 1833, 1820, 1788, 1818, 1778, 1803, 1779, 1737, 1657, 1624, 1650, 1606, 1615, 1618, 1660, 1624, 1641, 1728, 1725, 1713, 433, 434, 428, 276, 277, 440, 1339, 1337, 1316, 1317, 1318, 1315, 1322, 1329, 1350, 1353, 1347, 1346, 1329, 1323, 1331, 1327, 1333, 1318, 1318, 1316, 1323, 1320, 1334, 1325, 1322, 1323, 1322, 1326, 1324, 1317, 1313, 1317, 1311, 1332, 1329, 1347, 1353, 1345, 1325, 1326, 1324, 1312, 1315, 1307, 1313, 1314, 1312, 1321, 1589, 1268, 1654, 1676, 1662, 1674, 1667, 1683, 1671, 1654, 1643, 1643, 1643, 1643, 1638, 1641, 1638, 1641, 1638, 1654, 1707, 1725, 1744, 797, 813, 797, 793, 792, 802, 802, 796, 811, 811, 808, 824, 825, 839, 833, 837, 857, 863, 860, 857, 877, 878, 892, 898, 903, 889, 878, 877, 881, 861, 890, 891, 913, 915, 927, 923, 927, 924, 906, 903, 887, 900, 905, 921, 924, 906, 906, 908, 903, 905, 927, 943, 944, 946, 940, 948, 957, 956, 973, 983, 964, 988, 978, 973, 973, 983, 996, 1002, 1003, 981, 938, 913, 915, 914, 939, 954, 947, 931, 915, 884, 902, 904, 912, 920, 915, 934, 929, 919, 924, 928, 945, 939, 918, 914, 917, 903, 887, 888, 893, 870, 877, 852, 858, 873, 876, 868, 884, 884, 882, 866, 864, 882, 904, 905, 920, 932, 920, 925, 929, 926, 939, 1569, 1590, 925, 929, 905, 1077, 1094, 1109, 1104, 1094, 1087, 1093, 1077, 1089, 1088, 1087, 1101, 1094, 1091, 1098, 1097, 1087, 1087, 1087, 1080, 1105, 1114, 1109, 1063, 1067, 1062, 1077, 1072, 1096, 1092, 1099, 1119, 1106, 1111, 1124, 1113, 1103, 1097, 1105, 1116, 1118, 1134, 1139, 1131, 1135, 1123, 1121, 1108, 1109, 1100, 1109, 1126, 1124, 1125, 1107, 1102, 1101, 1086, 1089, 1110, 1110, 1103, 1113, 1125, 1125, 1130, 1134, 1149, 1150, 1165, 1155, 1151, 1155, 1158, 1134, 1135, 1140, 1145, 1141, 1156, 1153, 1168, 1164, 1168, 1179, 1181, 1171, 1176, 1207, 1202, 1200, 1231, 1235, 1232, 1231, 1222, 1223, 1192, 1182, 1176, 1191, 1202, 1212, 1211, 1212, 1216, 1225, 1223, 1206, 1219, 1240, 1235, 1231, 1216, 1226, 1221, 1233, 1230, 1245, 1259, 1231, 1238, 1235, 1243, 1253, 1237, 1239, 1242, 1197, 1211, 1209, 1191, 1203, 1197, 1212, 1195, 1211, 1209, 1208, 1193, 1184, 1191, 1184, 1195, 1189, 1181, 1162, 1163, 1167, 1181, 1181, 1185, 1195, 1194, 1204, 1201, 1204, 1198, 1214, 1214, 1214, 1221, 1224, 1225, 1230, 1230, 1229, 1225, 1229, 1240, 1236, 1237, 1236, 1248, 1244, 1252, 1248, 1248, 1254, 1253, 1244, 1245, 1245, 1268, 1268, 1276, 1277, 1279, 1286, 1292, 1292, 1298, 1295, 1309, 1310, 1310, 1318, 1314, 1341, 1349, 1332, 1346, 1354, 1350, 1372, 1380, 1371, 1369, 1334, 1346, 1326, 1331, 1339, 1339, 1325, 1344, 1361, 1351, 1357, 1348, 1392, 1394, 1410, 1392, 1403, 1409, 1366, 1360, 1340, 1336, 1350, 1354, 1366, 1377, 1373, 1374, 1364, 1368, 1385, 1415, 1423, 1401, 1382, 1387, 1383, 1393, 1396, 1380, 1371, 1376, 1385, 1385, 1385, 1380, 1384, 1383, 1405, 1411, 1406, 1421, 1414, 1380, 1377, 1372, 1369, 1374, 1381, 1388, 1373, 1359, 1361, 1368, 1370, 1345, 1343, 1334, 1333, 1346, 1312, 1336, 1332, 1340, 1336, 1353, 1348, 1350, 1364, 1363, 1363, 1359, 1362, 1373, 1374, 1383, 1388, 1406, 1399, 1411, 1402, 1408, 1410, 1434, 1435, 1416, 1428, 1431, 1428, 1429, 1412, 1422, 1429, 1396, 1401, 1405, 1426, 1427, 1429, 1439, 1432, 1427, 1422, 1418, 1424, 1432, 1428, 1437, 1456, 1460, 1460, 1473, 1462, 1452, 1459, 1476, 1486, 1497, 1497, 1503, 1507, 1511, 1505, 1506, 1527, 1536, 1567, 1545, 1537, 1516, 1474, 1493, 1513, 1513, 1500, 1506, 1493, 1491, 1486, 1498, 1495, 1512, 1518, 1527, 1524, 1523, 1537, 1539, 1535, 1542, 1535, 1544, 1543, 1545, 1538, 1545, 1530, 1516, 1523, 1530, 1528, 1538, 1540, 1548, 1549, 1522, 1500, 1496, 1501, 1512, 1502, 1486, 1496, 1517, 1530, 1533, 1544, 1555, 1566, 1582, 1588, 1595, 1605, 1589, 1598, 1592, 1599, 1614, 1618, 1613, 1616, 1628, 1620, 1659, 1661, 1648, 1662, 1717, 1743, 1796, 1763, 1746, 1766, 1786, 1789, 1824, 1859, 1897, 1830, 1750, 1772, 1837, 1787, 1835, 1823, 1824, 1877, 1903, 1872, 1816, 1868, 1857, 1813, 1833, 1820, 1788, 1818, 1778, 1803, 1779, 1737, 1657, 1624, 1650, 1606, 1615, 1618, 1660, 1624, 1641, 1644, 1654, 1676, 1662, 1674, 1667, 1683, 1671, 1654, 1643, 1618, 1641, 1654, 1707, 1725, 1744, 1744, 1716, 1720, 1736, 1763, 1753, 1794, 1785, 1770, 1759, 1788, 1780, 1781, 1762, 1722, 1723, 1679, 1700, 1692, 1694, 1680, 1712, 1714, 1746, 1745, 1746, 1722, 1728, 1742, 1708, 1711, 1666, 1632, 1574, 1570, 1599, 1594, 1615, 1615, 1606, 1607, 1607, 1593, 1556, 1545, 1566, 1566, 1602, 1611, 1621, 1616, 1611, 1632, 1640, 1650, 1639, 1643, 1652, 1659, 1657, 1666, 1676, 1665, 1710, 1720, 1738, 1729, 1738, 1744, 1760, 1726, 1719, 1745, 1733, 1731, 1725, 1722, 1719, 1727, 1728, 1723, 1734, 1759, 1776, 1780, 1773, 1767, 1784, 1696, 1717, 1711, 1705, 1674, 1685, 1700, 1709, 1699, 1675, 1642, 1659, 1654, 1666, 1651, 1652, 1645, 1661, 1690, 1680, 1661, 1661, 1668, 1676, 1644, 1620, 1628, 1641, 1641, 1659, 1659, 1675, 1654, 1652, 1651, 1643, 1643, 1643, 1643, 1638, 1641, 1645, 1657, 1662, 1666, 1662, 1653, 1636, 1641, 1638, 1605, 1589, 1593, 1580, 1557, 1546, 1540, 1574, 1593, 1593, 1569, 1563, 1558, 1574, 1574, 1554, 1563, 1560, 1627, 1619, 1617, 1620, 1589, 1593, 1596, 1609, 1617, 1622, 1627, 1628, 1618, 1605, 1564, 1572, 1583, 1572, 1573, 1551, 1599, 1598, 1617, 1617, 1604, 1580, 1591, 1565, 1576, 1571, 1591, 1591, 1584, 1573, 1582, 1582, 1580, 1581, 1605, 1616, 1625, 1621, 1599, 1280, 1282, 1543, 1545, 1538, 1545, 1530, 1516, 1523, 1530, 1528, 1538, 1540, 1548, 1549, 1522, 1500, 1496, 1501, 1512, 1502, 1486, 1496, 1517, 1530, 1533, 1544, 1555, 1566, 1582, 1588, 1595, 1605, 1589, 1598, 1592, 1599, 1614, 1618, 1613, 1616, 1628, 1620, 1659, 1661, 1648, 1662, 1717, 1743, 1796, 1763, 1746, 1766, 1786, 1789, 1824, 1859, 1897, 1830, 1750, 1772, 1837, 1787, 1835, 1823, 1824, 1859, 1897, 1830, 1750, 1772, 1837, 1787, 1835, 1823, 1824, 1877, 1903, 1872, 1816, 1868, 1857, 1813, 1833, 1820, 1788, 1818, 1778, 1803, 1779, 1737, 1657, 1624, 1650, 1606, 1615, 1618, 1660, 1624, 1641, 1644, 1654, 1676, 1662, 1674, 1667, 1683, 1671, 1654, 1643, 1618, 1641, 1654, 1707, 1725, 1744, 1744, 1716, 1720, 1736, 1763, 1753, 1794, 1785, 1770, 1759, 1788, 1780, 1781, 1762, 1722, 1723, 272, 272]\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(m.generate(context, max_new_tokens=2000)[0].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cc0eb4-ed86-4815-99cf-c1763b6b714d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
