{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "179d998c-ae40-43c0-b1c2-c9368244894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ecd60c-b30b-41cb-ab10-33be3783cab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fe85906-6193-48e4-8fc3-9b7543ba84b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Define the generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 3 * 64 * 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z).view(-1, 3, 64, 64)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6040151-859d-4c8c-afa7-e506b8f1d3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 8 * 8, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.model = self.model.to(device)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e6ec290-be3d-4adf-8e3b-361537b12c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f6e682687f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bae2f0b-0ba3-4bd5-b1b0-359fb719aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up data loaders\n",
    "def load_data(root_folder, batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.ImageFolder(root=root_folder, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    return dataloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e8cbefe-f31e-43ad-93c8-a2d0af112979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_gan(generator, discriminator, dataloader, num_epochs, batch_size, learning_rate):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    fixed_noise = torch.randn(64, 100).to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            real_imgs, _ = data\n",
    "            real_imgs = real_imgs.to(device)\n",
    "\n",
    " \n",
    "            # we first generate outputs from 'descriminator' by using real images and fake images (fake images are generated by 'generator' with noise as input) and then we calculate loss for fake and real image cases and then use it to train descriminator (losses from outputs of descriminator function for real images and fake images are used in backpropogation). Then, 'generator' is trained  by loss generated from discriminator' output of fake images\n",
    "            # Train discriminator with real images\n",
    "            optimizer_d.zero_grad()\n",
    "            real_outputs = discriminator(real_imgs)\n",
    "            real_labels = torch.ones(real_outputs.size(0), 1).to(device)  # Explicitly set the target shape\n",
    "            loss_real = criterion(real_outputs, real_labels)\n",
    "            loss_real.backward()\n",
    "            \n",
    "            # Train discriminator with fake images\n",
    "            noise = torch.randn(real_imgs.size(0), 100).cuda()\n",
    "            fake_imgs = generator(noise)\n",
    "            fake_outputs = discriminator(fake_imgs.detach())\n",
    "            fake_labels = torch.zeros(fake_outputs.size(0), 1).to(device)  # Explicitly set the target shape\n",
    "            loss_fake = criterion(fake_outputs, fake_labels)\n",
    "            loss_fake.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Train generator\n",
    "            optimizer_g.zero_grad()\n",
    "            gen_labels = torch.ones(fake_outputs.size(0), 1).to(device)  # Explicitly set the target shape\n",
    "            gen_outputs = discriminator(fake_imgs)\n",
    "            loss_gen = criterion(gen_outputs, gen_labels)\n",
    "            loss_gen.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f'\n",
    "                      % (epoch, num_epochs, i, len(dataloader),\n",
    "                         (loss_real + loss_fake).item(), loss_gen.item()))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            fake = generator(fixed_noise)\n",
    "            save_image(fake.detach(), 'fake_samples_epoch_%03d.png' % epoch, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8691f6f0-f2ca-4b43-9df2-7782c9c67301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][0/27] Loss_D: 1.5184 Loss_G: 4.6142\n",
      "[1/200][0/27] Loss_D: 0.6283 Loss_G: 5.4614\n",
      "[2/200][0/27] Loss_D: 0.3293 Loss_G: 5.0455\n",
      "[3/200][0/27] Loss_D: 0.3355 Loss_G: 4.9650\n",
      "[4/200][0/27] Loss_D: 0.1514 Loss_G: 5.1523\n",
      "[5/200][0/27] Loss_D: 0.2153 Loss_G: 3.2033\n",
      "[6/200][0/27] Loss_D: 0.2326 Loss_G: 3.4324\n",
      "[7/200][0/27] Loss_D: 0.2360 Loss_G: 4.3199\n",
      "[8/200][0/27] Loss_D: 0.1339 Loss_G: 4.7388\n",
      "[9/200][0/27] Loss_D: 0.1670 Loss_G: 4.1772\n",
      "[10/200][0/27] Loss_D: 0.2433 Loss_G: 4.5591\n",
      "[11/200][0/27] Loss_D: 0.2830 Loss_G: 3.7545\n",
      "[12/200][0/27] Loss_D: 1.1343 Loss_G: 4.8237\n",
      "[13/200][0/27] Loss_D: 0.4937 Loss_G: 3.0803\n",
      "[14/200][0/27] Loss_D: 0.5474 Loss_G: 4.5738\n",
      "[15/200][0/27] Loss_D: 0.6311 Loss_G: 3.1122\n",
      "[16/200][0/27] Loss_D: 0.6729 Loss_G: 6.4456\n",
      "[17/200][0/27] Loss_D: 0.2445 Loss_G: 4.2814\n",
      "[18/200][0/27] Loss_D: 0.2686 Loss_G: 4.9952\n",
      "[19/200][0/27] Loss_D: 0.4218 Loss_G: 3.5474\n",
      "[20/200][0/27] Loss_D: 0.9026 Loss_G: 8.5791\n",
      "[21/200][0/27] Loss_D: 0.6131 Loss_G: 4.2350\n",
      "[22/200][0/27] Loss_D: 0.4647 Loss_G: 4.1861\n",
      "[23/200][0/27] Loss_D: 0.5193 Loss_G: 5.7951\n",
      "[24/200][0/27] Loss_D: 0.7828 Loss_G: 5.1137\n",
      "[25/200][0/27] Loss_D: 0.3506 Loss_G: 4.2780\n",
      "[26/200][0/27] Loss_D: 0.3388 Loss_G: 3.8328\n",
      "[27/200][0/27] Loss_D: 0.1676 Loss_G: 6.0938\n",
      "[28/200][0/27] Loss_D: 0.1677 Loss_G: 5.2007\n",
      "[29/200][0/27] Loss_D: 0.5934 Loss_G: 7.3919\n",
      "[30/200][0/27] Loss_D: 0.6244 Loss_G: 4.3903\n",
      "[31/200][0/27] Loss_D: 0.2151 Loss_G: 4.5687\n",
      "[32/200][0/27] Loss_D: 0.3878 Loss_G: 4.9505\n",
      "[33/200][0/27] Loss_D: 0.2557 Loss_G: 3.8808\n",
      "[34/200][0/27] Loss_D: 0.4003 Loss_G: 4.9331\n",
      "[35/200][0/27] Loss_D: 0.2260 Loss_G: 4.5000\n",
      "[36/200][0/27] Loss_D: 0.1637 Loss_G: 5.1883\n",
      "[37/200][0/27] Loss_D: 0.3249 Loss_G: 4.0763\n",
      "[38/200][0/27] Loss_D: 0.8352 Loss_G: 5.2586\n",
      "[39/200][0/27] Loss_D: 0.3171 Loss_G: 5.2814\n",
      "[40/200][0/27] Loss_D: 0.4440 Loss_G: 4.1049\n",
      "[41/200][0/27] Loss_D: 0.2585 Loss_G: 5.5167\n",
      "[42/200][0/27] Loss_D: 0.5972 Loss_G: 8.3805\n",
      "[43/200][0/27] Loss_D: 0.3674 Loss_G: 4.6558\n",
      "[44/200][0/27] Loss_D: 0.1486 Loss_G: 4.7749\n",
      "[45/200][0/27] Loss_D: 0.2767 Loss_G: 6.2474\n",
      "[46/200][0/27] Loss_D: 0.3790 Loss_G: 4.3812\n",
      "[47/200][0/27] Loss_D: 0.4397 Loss_G: 4.5950\n",
      "[48/200][0/27] Loss_D: 0.2908 Loss_G: 6.4359\n",
      "[49/200][0/27] Loss_D: 0.2660 Loss_G: 4.3497\n",
      "[50/200][0/27] Loss_D: 0.2409 Loss_G: 5.1500\n",
      "[51/200][0/27] Loss_D: 0.1259 Loss_G: 5.4290\n",
      "[52/200][0/27] Loss_D: 0.1510 Loss_G: 4.0517\n",
      "[53/200][0/27] Loss_D: 0.0939 Loss_G: 5.0772\n",
      "[54/200][0/27] Loss_D: 0.7509 Loss_G: 13.7312\n",
      "[55/200][0/27] Loss_D: 0.4197 Loss_G: 5.1904\n",
      "[56/200][0/27] Loss_D: 0.2462 Loss_G: 3.8409\n",
      "[57/200][0/27] Loss_D: 0.3111 Loss_G: 3.9186\n",
      "[58/200][0/27] Loss_D: 0.2239 Loss_G: 4.3802\n",
      "[59/200][0/27] Loss_D: 0.4696 Loss_G: 5.9527\n",
      "[60/200][0/27] Loss_D: 0.1782 Loss_G: 4.2366\n",
      "[61/200][0/27] Loss_D: 0.1388 Loss_G: 6.6435\n",
      "[62/200][0/27] Loss_D: 0.1960 Loss_G: 4.2777\n",
      "[63/200][0/27] Loss_D: 0.1110 Loss_G: 5.4968\n",
      "[64/200][0/27] Loss_D: 0.1287 Loss_G: 4.1918\n",
      "[65/200][0/27] Loss_D: 0.2548 Loss_G: 8.1764\n",
      "[66/200][0/27] Loss_D: 0.1346 Loss_G: 4.4983\n",
      "[67/200][0/27] Loss_D: 0.4891 Loss_G: 6.3728\n",
      "[68/200][0/27] Loss_D: 0.0988 Loss_G: 7.3792\n",
      "[69/200][0/27] Loss_D: 0.1012 Loss_G: 4.9209\n",
      "[70/200][0/27] Loss_D: 0.2186 Loss_G: 6.0675\n",
      "[71/200][0/27] Loss_D: 0.4329 Loss_G: 4.3934\n",
      "[72/200][0/27] Loss_D: 0.1512 Loss_G: 5.3338\n",
      "[73/200][0/27] Loss_D: 0.1111 Loss_G: 6.0551\n",
      "[74/200][0/27] Loss_D: 0.1602 Loss_G: 4.3070\n",
      "[75/200][0/27] Loss_D: 0.1993 Loss_G: 4.3122\n",
      "[76/200][0/27] Loss_D: 0.7854 Loss_G: 12.7111\n",
      "[77/200][0/27] Loss_D: 0.3538 Loss_G: 4.4040\n",
      "[78/200][0/27] Loss_D: 0.3097 Loss_G: 4.5816\n",
      "[79/200][0/27] Loss_D: 0.2157 Loss_G: 3.4135\n",
      "[80/200][0/27] Loss_D: 0.3164 Loss_G: 4.1044\n",
      "[81/200][0/27] Loss_D: 0.1307 Loss_G: 4.3931\n",
      "[82/200][0/27] Loss_D: 0.2497 Loss_G: 3.8571\n",
      "[83/200][0/27] Loss_D: 0.4490 Loss_G: 7.2091\n",
      "[84/200][0/27] Loss_D: 0.1660 Loss_G: 6.4351\n",
      "[85/200][0/27] Loss_D: 0.2819 Loss_G: 5.9752\n",
      "[86/200][0/27] Loss_D: 0.2583 Loss_G: 5.8895\n",
      "[87/200][0/27] Loss_D: 0.4056 Loss_G: 7.9117\n",
      "[88/200][0/27] Loss_D: 1.0387 Loss_G: 14.5340\n",
      "[89/200][0/27] Loss_D: 0.1882 Loss_G: 4.9597\n",
      "[90/200][0/27] Loss_D: 0.0629 Loss_G: 5.6627\n",
      "[91/200][0/27] Loss_D: 0.0982 Loss_G: 5.1340\n",
      "[92/200][0/27] Loss_D: 0.1750 Loss_G: 4.2810\n",
      "[93/200][0/27] Loss_D: 0.2981 Loss_G: 3.0680\n",
      "[94/200][0/27] Loss_D: 0.0735 Loss_G: 6.9700\n",
      "[95/200][0/27] Loss_D: 0.2392 Loss_G: 6.0994\n",
      "[96/200][0/27] Loss_D: 0.4863 Loss_G: 7.5015\n",
      "[97/200][0/27] Loss_D: 0.5146 Loss_G: 6.5349\n",
      "[98/200][0/27] Loss_D: 0.4194 Loss_G: 3.2903\n",
      "[99/200][0/27] Loss_D: 0.2295 Loss_G: 4.1505\n",
      "[100/200][0/27] Loss_D: 0.0894 Loss_G: 4.0023\n",
      "[101/200][0/27] Loss_D: 0.3200 Loss_G: 2.9539\n",
      "[102/200][0/27] Loss_D: 0.1290 Loss_G: 4.0520\n",
      "[103/200][0/27] Loss_D: 0.1706 Loss_G: 3.6098\n",
      "[104/200][0/27] Loss_D: 0.2555 Loss_G: 3.4179\n",
      "[105/200][0/27] Loss_D: 0.2526 Loss_G: 3.5725\n",
      "[106/200][0/27] Loss_D: 0.3806 Loss_G: 5.3749\n",
      "[107/200][0/27] Loss_D: 0.3707 Loss_G: 5.2068\n",
      "[108/200][0/27] Loss_D: 0.2994 Loss_G: 4.7541\n",
      "[109/200][0/27] Loss_D: 0.1392 Loss_G: 3.8189\n",
      "[110/200][0/27] Loss_D: 0.6685 Loss_G: 4.5642\n",
      "[111/200][0/27] Loss_D: 0.2524 Loss_G: 3.7677\n",
      "[112/200][0/27] Loss_D: 0.0983 Loss_G: 5.1274\n",
      "[113/200][0/27] Loss_D: 0.3165 Loss_G: 4.5285\n",
      "[114/200][0/27] Loss_D: 0.3405 Loss_G: 3.0466\n",
      "[115/200][0/27] Loss_D: 0.4191 Loss_G: 4.5018\n",
      "[116/200][0/27] Loss_D: 0.2669 Loss_G: 3.8855\n",
      "[117/200][0/27] Loss_D: 0.2573 Loss_G: 4.7899\n",
      "[118/200][0/27] Loss_D: 0.2506 Loss_G: 2.9624\n",
      "[119/200][0/27] Loss_D: 0.3870 Loss_G: 4.2986\n",
      "[120/200][0/27] Loss_D: 0.2723 Loss_G: 3.6300\n",
      "[121/200][0/27] Loss_D: 0.2361 Loss_G: 3.5823\n",
      "[122/200][0/27] Loss_D: 0.3900 Loss_G: 5.0364\n",
      "[123/200][0/27] Loss_D: 0.3775 Loss_G: 4.7208\n",
      "[124/200][0/27] Loss_D: 0.2355 Loss_G: 3.9373\n",
      "[125/200][0/27] Loss_D: 0.2468 Loss_G: 4.1562\n",
      "[126/200][0/27] Loss_D: 0.3324 Loss_G: 6.4049\n",
      "[127/200][0/27] Loss_D: 0.5485 Loss_G: 5.6987\n",
      "[128/200][0/27] Loss_D: 0.4278 Loss_G: 4.1756\n",
      "[129/200][0/27] Loss_D: 0.4226 Loss_G: 4.0708\n",
      "[130/200][0/27] Loss_D: 0.7108 Loss_G: 2.4897\n",
      "[131/200][0/27] Loss_D: 0.5990 Loss_G: 4.7234\n",
      "[132/200][0/27] Loss_D: 0.2546 Loss_G: 3.3711\n",
      "[133/200][0/27] Loss_D: 0.8089 Loss_G: 3.6834\n",
      "[134/200][0/27] Loss_D: 0.5229 Loss_G: 3.9258\n",
      "[135/200][0/27] Loss_D: 0.4433 Loss_G: 3.3001\n",
      "[136/200][0/27] Loss_D: 0.6116 Loss_G: 3.6705\n",
      "[137/200][0/27] Loss_D: 0.5648 Loss_G: 2.8432\n",
      "[138/200][0/27] Loss_D: 0.4453 Loss_G: 2.4630\n",
      "[139/200][0/27] Loss_D: 0.6529 Loss_G: 2.6743\n",
      "[140/200][0/27] Loss_D: 0.5039 Loss_G: 4.2773\n",
      "[141/200][0/27] Loss_D: 0.3235 Loss_G: 4.2282\n",
      "[142/200][0/27] Loss_D: 0.3331 Loss_G: 5.2489\n",
      "[143/200][0/27] Loss_D: 0.9776 Loss_G: 8.1252\n",
      "[144/200][0/27] Loss_D: 0.3731 Loss_G: 4.4188\n",
      "[145/200][0/27] Loss_D: 0.2542 Loss_G: 3.2003\n",
      "[146/200][0/27] Loss_D: 0.4172 Loss_G: 5.9621\n",
      "[147/200][0/27] Loss_D: 0.9544 Loss_G: 6.0121\n",
      "[148/200][0/27] Loss_D: 0.7500 Loss_G: 3.4458\n",
      "[149/200][0/27] Loss_D: 0.8565 Loss_G: 3.9571\n",
      "[150/200][0/27] Loss_D: 0.3478 Loss_G: 4.1896\n",
      "[151/200][0/27] Loss_D: 0.8538 Loss_G: 5.0847\n",
      "[152/200][0/27] Loss_D: 0.4991 Loss_G: 3.9990\n",
      "[153/200][0/27] Loss_D: 0.3152 Loss_G: 2.8371\n",
      "[154/200][0/27] Loss_D: 0.4668 Loss_G: 2.3376\n",
      "[155/200][0/27] Loss_D: 0.4003 Loss_G: 2.2964\n",
      "[156/200][0/27] Loss_D: 0.4167 Loss_G: 3.3969\n",
      "[157/200][0/27] Loss_D: 0.6623 Loss_G: 3.9436\n",
      "[158/200][0/27] Loss_D: 0.2762 Loss_G: 3.8893\n",
      "[159/200][0/27] Loss_D: 0.3042 Loss_G: 4.5850\n",
      "[160/200][0/27] Loss_D: 0.5856 Loss_G: 3.6799\n",
      "[161/200][0/27] Loss_D: 0.6369 Loss_G: 3.2231\n",
      "[162/200][0/27] Loss_D: 0.4830 Loss_G: 4.2464\n",
      "[163/200][0/27] Loss_D: 0.2685 Loss_G: 3.8426\n",
      "[164/200][0/27] Loss_D: 0.4562 Loss_G: 2.4885\n",
      "[165/200][0/27] Loss_D: 0.2825 Loss_G: 3.2917\n",
      "[166/200][0/27] Loss_D: 0.4446 Loss_G: 3.6713\n",
      "[167/200][0/27] Loss_D: 0.6226 Loss_G: 5.2324\n",
      "[168/200][0/27] Loss_D: 0.3272 Loss_G: 3.4753\n",
      "[169/200][0/27] Loss_D: 0.2343 Loss_G: 4.0458\n",
      "[170/200][0/27] Loss_D: 0.3567 Loss_G: 4.7548\n",
      "[171/200][0/27] Loss_D: 0.4647 Loss_G: 4.4785\n",
      "[172/200][0/27] Loss_D: 0.2068 Loss_G: 4.0620\n",
      "[173/200][0/27] Loss_D: 0.5445 Loss_G: 5.1527\n",
      "[174/200][0/27] Loss_D: 0.5133 Loss_G: 4.4289\n",
      "[175/200][0/27] Loss_D: 0.3523 Loss_G: 3.9713\n",
      "[176/200][0/27] Loss_D: 0.2246 Loss_G: 4.0351\n",
      "[177/200][0/27] Loss_D: 0.5421 Loss_G: 4.2204\n",
      "[178/200][0/27] Loss_D: 0.3509 Loss_G: 3.0319\n",
      "[179/200][0/27] Loss_D: 0.1722 Loss_G: 4.5319\n",
      "[180/200][0/27] Loss_D: 0.4961 Loss_G: 3.8016\n",
      "[181/200][0/27] Loss_D: 0.2481 Loss_G: 3.6949\n",
      "[182/200][0/27] Loss_D: 0.1851 Loss_G: 4.0084\n",
      "[183/200][0/27] Loss_D: 0.5799 Loss_G: 4.4374\n",
      "[184/200][0/27] Loss_D: 0.3130 Loss_G: 4.1097\n",
      "[185/200][0/27] Loss_D: 0.6401 Loss_G: 6.8339\n",
      "[186/200][0/27] Loss_D: 0.7979 Loss_G: 3.3754\n",
      "[187/200][0/27] Loss_D: 0.7064 Loss_G: 5.0651\n",
      "[188/200][0/27] Loss_D: 0.6758 Loss_G: 4.5586\n",
      "[189/200][0/27] Loss_D: 0.5347 Loss_G: 5.2612\n",
      "[190/200][0/27] Loss_D: 0.2730 Loss_G: 4.2916\n",
      "[191/200][0/27] Loss_D: 0.8736 Loss_G: 7.3164\n",
      "[192/200][0/27] Loss_D: 0.2713 Loss_G: 5.5229\n",
      "[193/200][0/27] Loss_D: 0.3299 Loss_G: 3.3691\n",
      "[194/200][0/27] Loss_D: 0.6685 Loss_G: 6.9648\n",
      "[195/200][0/27] Loss_D: 0.4443 Loss_G: 3.7226\n",
      "[196/200][0/27] Loss_D: 0.3835 Loss_G: 4.6956\n",
      "[197/200][0/27] Loss_D: 0.3912 Loss_G: 5.1155\n",
      "[198/200][0/27] Loss_D: 0.8486 Loss_G: 8.2096\n",
      "[199/200][0/27] Loss_D: 0.7092 Loss_G: 5.3585\n"
     ]
    }
   ],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set up data loaders for cat and dog images\n",
    "cat_dataloader = load_data('/home/tom/Python/Machine learning/pytorch/data/cats/', batch_size=64)\n",
    "#dog_dataloader = load_data('/home/tom/Python/Machine learning/pytorch/GaN/data/dogs', batch_size=64)\n",
    "\n",
    "# Initialize the generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "generator.to(device)\n",
    "discriminator.to(device)\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator, discriminator, cat_dataloader, num_epochs=200, batch_size=64, learning_rate=0.0002)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b892a-e372-458f-8f5a-eb553ae8d512",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
