{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b009e15-03ad-4ff7-bd3a-d26c842e361a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/200][0/24] Loss_D: 1.3782 Loss_G: 1.1221\n",
      "[1/200][0/24] Loss_D: 0.7534 Loss_G: 1.0054\n",
      "[2/200][0/24] Loss_D: 0.6284 Loss_G: 1.4632\n",
      "[3/200][0/24] Loss_D: 0.7423 Loss_G: 2.4168\n",
      "[4/200][0/24] Loss_D: 0.3863 Loss_G: 2.2779\n",
      "[5/200][0/24] Loss_D: 0.4120 Loss_G: 1.8583\n",
      "[6/200][0/24] Loss_D: 0.4492 Loss_G: 1.6300\n",
      "[7/200][0/24] Loss_D: 0.5057 Loss_G: 1.5529\n",
      "[8/200][0/24] Loss_D: 0.6584 Loss_G: 1.1120\n",
      "[9/200][0/24] Loss_D: 0.5071 Loss_G: 1.3675\n",
      "[10/200][0/24] Loss_D: 0.4760 Loss_G: 1.4471\n",
      "[11/200][0/24] Loss_D: 0.4506 Loss_G: 1.4142\n",
      "[12/200][0/24] Loss_D: 0.4286 Loss_G: 1.4060\n",
      "[13/200][0/24] Loss_D: 0.4439 Loss_G: 1.5326\n",
      "[14/200][0/24] Loss_D: 0.3445 Loss_G: 1.6963\n",
      "[15/200][0/24] Loss_D: 0.3220 Loss_G: 1.7178\n",
      "[16/200][0/24] Loss_D: 0.3120 Loss_G: 1.6725\n",
      "[17/200][0/24] Loss_D: 0.3102 Loss_G: 1.7859\n",
      "[18/200][0/24] Loss_D: 0.2809 Loss_G: 1.9249\n",
      "[19/200][0/24] Loss_D: 0.2918 Loss_G: 1.9887\n",
      "[20/200][0/24] Loss_D: 0.2669 Loss_G: 2.0759\n",
      "[21/200][0/24] Loss_D: 0.3397 Loss_G: 1.8341\n",
      "[22/200][0/24] Loss_D: 0.2552 Loss_G: 2.0774\n",
      "[23/200][0/24] Loss_D: 0.2641 Loss_G: 1.9990\n",
      "[24/200][0/24] Loss_D: 0.2539 Loss_G: 2.0015\n",
      "[25/200][0/24] Loss_D: 0.2312 Loss_G: 2.0630\n",
      "[26/200][0/24] Loss_D: 0.2525 Loss_G: 1.9113\n",
      "[27/200][0/24] Loss_D: 0.2522 Loss_G: 1.8848\n",
      "[28/200][0/24] Loss_D: 0.2440 Loss_G: 1.9269\n",
      "[29/200][0/24] Loss_D: 0.2452 Loss_G: 1.8793\n",
      "[30/200][0/24] Loss_D: 0.2295 Loss_G: 1.9242\n",
      "[31/200][0/24] Loss_D: 0.2205 Loss_G: 1.9753\n",
      "[32/200][0/24] Loss_D: 0.2165 Loss_G: 1.9563\n",
      "[33/200][0/24] Loss_D: 0.2539 Loss_G: 1.8211\n",
      "[34/200][0/24] Loss_D: 0.1927 Loss_G: 2.0721\n",
      "[35/200][0/24] Loss_D: 0.1775 Loss_G: 2.1460\n",
      "[36/200][0/24] Loss_D: 0.1933 Loss_G: 2.0269\n",
      "[37/200][0/24] Loss_D: 0.1715 Loss_G: 2.1067\n",
      "[38/200][0/24] Loss_D: 0.1591 Loss_G: 2.1640\n",
      "[39/200][0/24] Loss_D: 0.1623 Loss_G: 2.1203\n",
      "[40/200][0/24] Loss_D: 0.1507 Loss_G: 2.2858\n",
      "[41/200][0/24] Loss_D: 0.1462 Loss_G: 2.2818\n",
      "[42/200][0/24] Loss_D: 0.1447 Loss_G: 2.4428\n",
      "[43/200][0/24] Loss_D: 0.1393 Loss_G: 2.3671\n",
      "[44/200][0/24] Loss_D: 0.1765 Loss_G: 2.4021\n",
      "[45/200][0/24] Loss_D: 0.1496 Loss_G: 2.5567\n",
      "[46/200][0/24] Loss_D: 0.1228 Loss_G: 2.7402\n",
      "[47/200][0/24] Loss_D: 0.1233 Loss_G: 2.8073\n",
      "[48/200][0/24] Loss_D: 0.1133 Loss_G: 2.8436\n",
      "[49/200][0/24] Loss_D: 0.1524 Loss_G: 2.6454\n",
      "[50/200][0/24] Loss_D: 0.0943 Loss_G: 2.8984\n",
      "[51/200][0/24] Loss_D: 0.1070 Loss_G: 2.9914\n",
      "[52/200][0/24] Loss_D: 0.0974 Loss_G: 2.7721\n",
      "[53/200][0/24] Loss_D: 0.0949 Loss_G: 2.8753\n",
      "[54/200][0/24] Loss_D: 0.0946 Loss_G: 2.9078\n",
      "[55/200][0/24] Loss_D: 0.0858 Loss_G: 2.9334\n",
      "[56/200][0/24] Loss_D: 0.0805 Loss_G: 3.0546\n",
      "[57/200][0/24] Loss_D: 0.0707 Loss_G: 3.3587\n",
      "[58/200][0/24] Loss_D: 0.0730 Loss_G: 3.1613\n",
      "[59/200][0/24] Loss_D: 0.0778 Loss_G: 3.1334\n",
      "[60/200][0/24] Loss_D: 0.0812 Loss_G: 3.0319\n",
      "[61/200][0/24] Loss_D: 0.0907 Loss_G: 3.1103\n",
      "[62/200][0/24] Loss_D: 0.0672 Loss_G: 3.2863\n",
      "[63/200][0/24] Loss_D: 0.0871 Loss_G: 3.1980\n",
      "[64/200][0/24] Loss_D: 0.0779 Loss_G: 3.3929\n",
      "[65/200][0/24] Loss_D: 0.0728 Loss_G: 3.3403\n",
      "[66/200][0/24] Loss_D: 0.0628 Loss_G: 3.4977\n",
      "[67/200][0/24] Loss_D: 0.0996 Loss_G: 3.2827\n",
      "[68/200][0/24] Loss_D: 0.0884 Loss_G: 3.4601\n",
      "[69/200][0/24] Loss_D: 0.0856 Loss_G: 3.2912\n",
      "[70/200][0/24] Loss_D: 0.0861 Loss_G: 3.3681\n",
      "[71/200][0/24] Loss_D: 0.0874 Loss_G: 3.3470\n",
      "[72/200][0/24] Loss_D: 0.0952 Loss_G: 3.4238\n",
      "[73/200][0/24] Loss_D: 0.1875 Loss_G: 3.3459\n",
      "[74/200][0/24] Loss_D: 0.0690 Loss_G: 3.8156\n",
      "[75/200][0/24] Loss_D: 0.0832 Loss_G: 3.6424\n",
      "[76/200][0/24] Loss_D: 0.1162 Loss_G: 3.7699\n",
      "[77/200][0/24] Loss_D: 0.0907 Loss_G: 3.7467\n",
      "[78/200][0/24] Loss_D: 0.0972 Loss_G: 3.9730\n",
      "[79/200][0/24] Loss_D: 0.0634 Loss_G: 3.9239\n",
      "[80/200][0/24] Loss_D: 0.0661 Loss_G: 4.1472\n",
      "[81/200][0/24] Loss_D: 0.0664 Loss_G: 3.9067\n",
      "[82/200][0/24] Loss_D: 0.1296 Loss_G: 4.2327\n",
      "[83/200][0/24] Loss_D: 0.0416 Loss_G: 4.4768\n",
      "[84/200][0/24] Loss_D: 0.1187 Loss_G: 3.8144\n",
      "[85/200][0/24] Loss_D: 0.1045 Loss_G: 4.1405\n",
      "[86/200][0/24] Loss_D: 0.2277 Loss_G: 4.1680\n",
      "[87/200][0/24] Loss_D: 0.2394 Loss_G: 4.2015\n",
      "[88/200][0/24] Loss_D: 0.1836 Loss_G: 4.3797\n",
      "[89/200][0/24] Loss_D: 0.3614 Loss_G: 3.6712\n",
      "[90/200][0/24] Loss_D: 0.1902 Loss_G: 4.4461\n",
      "[91/200][0/24] Loss_D: 0.0575 Loss_G: 4.8307\n",
      "[92/200][0/24] Loss_D: 0.0709 Loss_G: 4.5326\n",
      "[93/200][0/24] Loss_D: 0.0539 Loss_G: 4.3413\n",
      "[94/200][0/24] Loss_D: 0.0478 Loss_G: 4.4835\n",
      "[95/200][0/24] Loss_D: 0.0678 Loss_G: 4.4328\n",
      "[96/200][0/24] Loss_D: 0.0455 Loss_G: 4.3150\n",
      "[97/200][0/24] Loss_D: 0.0397 Loss_G: 4.8130\n",
      "[98/200][0/24] Loss_D: 0.0725 Loss_G: 4.6576\n",
      "[99/200][0/24] Loss_D: 0.0612 Loss_G: 4.6428\n",
      "[100/200][0/24] Loss_D: 0.0357 Loss_G: 4.9728\n",
      "[101/200][0/24] Loss_D: 0.1393 Loss_G: 4.5744\n",
      "[102/200][0/24] Loss_D: 0.0914 Loss_G: 3.9599\n",
      "[103/200][0/24] Loss_D: 0.0730 Loss_G: 4.2522\n",
      "[104/200][0/24] Loss_D: 0.0641 Loss_G: 4.2260\n",
      "[105/200][0/24] Loss_D: 0.1013 Loss_G: 4.6034\n",
      "[106/200][0/24] Loss_D: 0.0910 Loss_G: 4.1837\n",
      "[107/200][0/24] Loss_D: 0.2190 Loss_G: 4.9650\n",
      "[108/200][0/24] Loss_D: 0.0513 Loss_G: 9.1716\n",
      "[109/200][0/24] Loss_D: 0.2400 Loss_G: 4.2723\n",
      "[110/200][0/24] Loss_D: 0.1649 Loss_G: 4.5424\n",
      "[111/200][0/24] Loss_D: 0.2683 Loss_G: 5.0053\n",
      "[112/200][0/24] Loss_D: 0.0784 Loss_G: 4.5538\n",
      "[113/200][0/24] Loss_D: 0.1334 Loss_G: 4.0975\n",
      "[114/200][0/24] Loss_D: 0.1396 Loss_G: 4.9390\n",
      "[115/200][0/24] Loss_D: 0.2646 Loss_G: 3.8769\n",
      "[116/200][0/24] Loss_D: 0.3439 Loss_G: 5.8004\n",
      "[117/200][0/24] Loss_D: 0.3549 Loss_G: 5.3442\n",
      "[118/200][0/24] Loss_D: 0.1487 Loss_G: 4.3725\n",
      "[119/200][0/24] Loss_D: 0.1204 Loss_G: 5.4835\n",
      "[120/200][0/24] Loss_D: 0.4167 Loss_G: 4.4518\n",
      "[121/200][0/24] Loss_D: 0.2284 Loss_G: 4.3160\n",
      "[122/200][0/24] Loss_D: 0.1208 Loss_G: 3.4736\n",
      "[123/200][0/24] Loss_D: 0.0632 Loss_G: 6.0288\n",
      "[124/200][0/24] Loss_D: 0.0949 Loss_G: 4.6003\n",
      "[125/200][0/24] Loss_D: 0.1387 Loss_G: 4.5289\n",
      "[126/200][0/24] Loss_D: 0.3063 Loss_G: 4.8694\n",
      "[127/200][0/24] Loss_D: 0.0678 Loss_G: 4.3880\n",
      "[128/200][0/24] Loss_D: 0.0603 Loss_G: 4.8332\n",
      "[129/200][0/24] Loss_D: 0.2090 Loss_G: 5.9918\n",
      "[130/200][0/24] Loss_D: 0.1194 Loss_G: 5.1661\n",
      "[131/200][0/24] Loss_D: 0.2112 Loss_G: 6.8598\n",
      "[132/200][0/24] Loss_D: 0.2173 Loss_G: 4.9787\n",
      "[133/200][0/24] Loss_D: 0.6787 Loss_G: 4.6032\n",
      "[134/200][0/24] Loss_D: 0.1066 Loss_G: 4.6766\n",
      "[135/200][0/24] Loss_D: 0.4021 Loss_G: 5.7274\n",
      "[136/200][0/24] Loss_D: 0.2649 Loss_G: 5.0968\n",
      "[137/200][0/24] Loss_D: 0.1989 Loss_G: 4.3019\n",
      "[138/200][0/24] Loss_D: 0.2100 Loss_G: 5.1860\n",
      "[139/200][0/24] Loss_D: 0.1730 Loss_G: 6.7887\n",
      "[140/200][0/24] Loss_D: 0.1634 Loss_G: 5.0902\n",
      "[141/200][0/24] Loss_D: 0.1165 Loss_G: 4.7391\n",
      "[142/200][0/24] Loss_D: 0.1709 Loss_G: 5.4674\n",
      "[143/200][0/24] Loss_D: 0.0845 Loss_G: 5.1756\n",
      "[144/200][0/24] Loss_D: 0.1184 Loss_G: 4.9478\n",
      "[145/200][0/24] Loss_D: 0.1564 Loss_G: 4.6517\n",
      "[146/200][0/24] Loss_D: 0.2691 Loss_G: 4.7911\n",
      "[147/200][0/24] Loss_D: 1.1690 Loss_G: 3.7347\n",
      "[148/200][0/24] Loss_D: 0.1801 Loss_G: 5.5645\n",
      "[149/200][0/24] Loss_D: 0.7523 Loss_G: 6.9445\n",
      "[150/200][0/24] Loss_D: 0.4303 Loss_G: 4.9454\n",
      "[151/200][0/24] Loss_D: 0.2021 Loss_G: 4.8891\n",
      "[152/200][0/24] Loss_D: 0.2708 Loss_G: 4.1270\n",
      "[153/200][0/24] Loss_D: 0.2415 Loss_G: 4.2228\n",
      "[154/200][0/24] Loss_D: 0.1780 Loss_G: 4.8523\n",
      "[155/200][0/24] Loss_D: 0.2118 Loss_G: 5.4892\n",
      "[156/200][0/24] Loss_D: 0.1719 Loss_G: 5.3739\n",
      "[157/200][0/24] Loss_D: 0.1914 Loss_G: 5.7593\n",
      "[158/200][0/24] Loss_D: 0.2916 Loss_G: 4.5757\n",
      "[159/200][0/24] Loss_D: 0.1695 Loss_G: 5.4515\n",
      "[160/200][0/24] Loss_D: 0.1312 Loss_G: 4.5520\n",
      "[161/200][0/24] Loss_D: 0.2937 Loss_G: 3.5927\n",
      "[162/200][0/24] Loss_D: 0.0952 Loss_G: 4.0346\n",
      "[163/200][0/24] Loss_D: 0.1573 Loss_G: 5.2450\n",
      "[164/200][0/24] Loss_D: 0.1779 Loss_G: 5.4151\n",
      "[165/200][0/24] Loss_D: 0.0727 Loss_G: 5.5827\n",
      "[166/200][0/24] Loss_D: 0.1460 Loss_G: 5.1117\n",
      "[167/200][0/24] Loss_D: 0.7765 Loss_G: 6.7197\n",
      "[168/200][0/24] Loss_D: 0.8370 Loss_G: 7.0321\n",
      "[169/200][0/24] Loss_D: 0.1002 Loss_G: 4.5112\n",
      "[170/200][0/24] Loss_D: 0.1892 Loss_G: 4.7389\n",
      "[171/200][0/24] Loss_D: 0.1363 Loss_G: 6.7568\n",
      "[172/200][0/24] Loss_D: 0.2090 Loss_G: 5.4389\n",
      "[173/200][0/24] Loss_D: 0.2091 Loss_G: 4.3939\n",
      "[174/200][0/24] Loss_D: 0.2360 Loss_G: 4.6647\n",
      "[175/200][0/24] Loss_D: 0.4928 Loss_G: 6.3142\n",
      "[176/200][0/24] Loss_D: 0.1542 Loss_G: 4.8028\n",
      "[177/200][0/24] Loss_D: 0.2054 Loss_G: 4.4503\n",
      "[178/200][0/24] Loss_D: 0.1820 Loss_G: 5.3256\n",
      "[179/200][0/24] Loss_D: 0.5322 Loss_G: 4.0221\n",
      "[180/200][0/24] Loss_D: 0.3723 Loss_G: 6.9420\n",
      "[181/200][0/24] Loss_D: 0.3310 Loss_G: 3.6689\n",
      "[182/200][0/24] Loss_D: 0.1803 Loss_G: 6.4963\n",
      "[183/200][0/24] Loss_D: 0.2846 Loss_G: 5.8627\n",
      "[184/200][0/24] Loss_D: 0.1300 Loss_G: 4.9273\n",
      "[185/200][0/24] Loss_D: 0.2065 Loss_G: 6.3109\n",
      "[186/200][0/24] Loss_D: 0.2567 Loss_G: 4.9860\n",
      "[187/200][0/24] Loss_D: 0.2225 Loss_G: 5.0317\n",
      "[188/200][0/24] Loss_D: 0.3741 Loss_G: 4.7199\n",
      "[189/200][0/24] Loss_D: 0.1308 Loss_G: 5.3372\n",
      "[190/200][0/24] Loss_D: 0.4006 Loss_G: 4.8386\n",
      "[191/200][0/24] Loss_D: 0.3155 Loss_G: 6.5529\n",
      "[192/200][0/24] Loss_D: 0.2256 Loss_G: 5.5430\n",
      "[193/200][0/24] Loss_D: 0.1216 Loss_G: 6.2195\n",
      "[194/200][0/24] Loss_D: 0.4819 Loss_G: 8.3234\n",
      "[195/200][0/24] Loss_D: 0.2670 Loss_G: 5.6452\n",
      "[196/200][0/24] Loss_D: 0.2289 Loss_G: 4.8061\n",
      "[197/200][0/24] Loss_D: 0.4406 Loss_G: 6.6346\n",
      "[198/200][0/24] Loss_D: 0.4716 Loss_G: 7.2628\n",
      "[199/200][0/24] Loss_D: 0.2293 Loss_G: 7.5451\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "# Define the generator network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(100, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 3 * 64 * 64),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.model = self.model.cuda()\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z).view(-1, 3, 64, 64)\n",
    "\n",
    "# Define the discriminator network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(3 * 64 * 64, 256),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.model = self.model.cuda()\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img.view(img.size(0), -1))\n",
    "\n",
    "# Set up data loaders\n",
    "def load_data(root_folder, batch_size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "    dataset = datasets.ImageFolder(root=root_folder, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# Training function\n",
    "def train_gan(generator, discriminator, dataloader, num_epochs, batch_size, learning_rate):\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer_g = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    optimizer_d = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "\n",
    "    fixed_noise = torch.randn(64, 100).cuda()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            real_imgs, _ = data\n",
    "            real_imgs = real_imgs.cuda()\n",
    "\n",
    " \n",
    "            # we first generate outputs from 'descriminator' by using real images and fake images (fake images are generated by 'generator' with noise as input) and then we calculate loss for fake and real image cases and then use it to train descriminator. Then, 'generator' is trained  by discriminator' output generated from fake images\n",
    "            # Train discriminator with real images\n",
    "            optimizer_d.zero_grad()\n",
    "            real_outputs = discriminator(real_imgs)\n",
    "            real_labels = torch.ones(real_outputs.size(0), 1).cuda()  # Explicitly set the target shape\n",
    "            loss_real = criterion(real_outputs, real_labels)\n",
    "            loss_real.backward()\n",
    "            \n",
    "            # Train discriminator with fake images\n",
    "            noise = torch.randn(real_imgs.size(0), 100).cuda()\n",
    "            fake_imgs = generator(noise)\n",
    "            fake_outputs = discriminator(fake_imgs.detach())\n",
    "            fake_labels = torch.zeros(fake_outputs.size(0), 1).cuda()  # Explicitly set the target shape\n",
    "            loss_fake = criterion(fake_outputs, fake_labels)\n",
    "            loss_fake.backward()\n",
    "            optimizer_d.step()\n",
    "            \n",
    "            # Train generator\n",
    "            optimizer_g.zero_grad()\n",
    "            gen_labels = torch.ones(fake_outputs.size(0), 1).cuda()  # Explicitly set the target shape\n",
    "            gen_outputs = discriminator(fake_imgs)\n",
    "            loss_gen = criterion(gen_outputs, gen_labels)\n",
    "            loss_gen.backward()\n",
    "            optimizer_g.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f'\n",
    "                      % (epoch, num_epochs, i, len(dataloader),\n",
    "                         (loss_real + loss_fake).item(), loss_gen.item()))\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            fake = generator(fixed_noise)\n",
    "            save_image(fake.detach(), 'fake_samples_epoch_%03d.png' % epoch, normalize=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Set up data loaders for cat and dog images\n",
    "#cat_dataloader = load_data('/home/tom/Python/Machine learning/pytorch/GaN/data/cat/', batch_size=64)\n",
    "dog_dataloader = load_data('/home/tom/Python/Machine learning/pytorch/GaN/data/dogs', batch_size=64)\n",
    "\n",
    "# Initialize the generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "\n",
    "# Train the GAN\n",
    "train_gan(generator, discriminator, dog_dataloader, num_epochs=200, batch_size=64, learning_rate=0.0002)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2599cb-f2fc-4f7e-b86d-6a5bdd840b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8549476c-eaea-4b19-9e53-2fec0fbf30de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
