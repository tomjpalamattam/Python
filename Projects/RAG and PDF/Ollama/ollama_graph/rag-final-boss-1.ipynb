{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e801c106-2c79-40fc-81ac-ec9921a6e1a7",
   "metadata": {},
   "source": [
    "# LANGCHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c5534af-e7e5-4138-936d-f0ccbb559400",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import START, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "497d2e6b-bc28-4cc9-a54a-ce6a77501a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#note: pypdf also records title, page numeber etc required by chain-5\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "\n",
    "#loader = DirectoryLoader('/home/tom/Python/Tools/RAG and PDF/docs', glob=\"**/*.pdf\", loader_cls=UnstructuredPDFLoader, show_progress=True)\n",
    "loader = DirectoryLoader('docs', glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader, show_progress=True)\n",
    "repo_files = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b22ab9-3f2b-4345-b6de-152f3c55af58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files loaded: 13\n",
      "Number of documents : 63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Number of files loaded: {len(repo_files)}\")\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "documents = text_splitter.split_documents(documents=repo_files)\n",
    "print(f\"Number of documents : {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5528a1d9-04fb-4b23-bf15-2bcaa05833bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "#from langchain_community.vectorstores import Qdrant\n",
    "from langchain_qdrant import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ad982c8-ad89-4195-9678-4aa7cb56c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nomic-embed-text\"\n",
    "embeddings = OllamaEmbeddings(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "156b80fc-aa42-416a-aeb7-3af00b3a8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore db\n",
    "\n",
    "qdrant = Qdrant.from_existing_collection(\n",
    " embeddings,\n",
    " path=\"langchain_local_qdrant_pdf_orig\",\n",
    " collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c3f0f4-77ee-4d23-b7bc-a42c943c490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fe4f782-6a58-48fa-913a-cd851818596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "api_key = 'PUT THE KEY HERE'\n",
    "#os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc055ed-87ec-4255-b204-f1842b1225d6",
   "metadata": {},
   "source": [
    "### use it as an llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4347a269-3298-4755-abcb-e811dcc76893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "response = requests.post(\n",
    "    url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "    headers={\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "    },\n",
    "    data=json.dumps({\n",
    "        \"model\": \"deepseek/deepseek-r1-distill-llama-70b:free\",  # Optional\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"What is the meaning of life?\"\n",
    "            }\n",
    "        ],\n",
    "        \"provider\": {\n",
    "            \"order\": [\n",
    "                \"Together\",\n",
    "                \"Targon\"              \n",
    "            ]\n",
    "        }\n",
    "    })\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c2e2cd6-3283-43a5-97cc-1e1b226cfcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The meaning of life is a deeply personal and subjective question, with answers varying widely among individuals. Here are the key points that encapsulate the exploration:\n",
      "\n",
      "1. **Diverse Perspectives**: Different people have varied beliefs, ranging from religious views, scientific explanations, to philosophical stances like existentialism, which emphasizes self-defined meaning.\n",
      "\n",
      "2. **Personal Fulfillment**: Each individual may find their own meaning through personal growth, relationships, career, or hobbies, suggesting that life's meaning is not universal but unique to each person.\n",
      "\n",
      "3. **Evolutionary View**: From a scientific perspective, life's purpose might be tied to survival and reproduction, yet humans often seek more complex forms of meaning beyond basic instincts.\n",
      "\n",
      "4. **Dynamic Meaning**: The meaning of life can evolve over time, influenced by different life stages and experiences, shifting from growth and learning to legacy and contribution.\n",
      "\n",
      "5. **Happiness and Contentment**: While happiness is a common source, it varies individually, and circumstances can affect its pursuit, suggesting adaptability in finding meaning.\n",
      "\n",
      "6. **Freedom and Responsibility**: Without an inherent meaning, individuals have the freedom to create their own, which can be both empowering and challenging.\n",
      "\n",
      "7. **Self-Reflection and Engagement**: Finding meaning might involve setting goals, building relationships, contributing to society, and engaging in challenging activities that foster growth.\n",
      "\n",
      "In conclusion, the meaning of life is not static or universal. It is a dynamic, personal journey where each individual shapes their own meaning through experiences, values, and aspirations.\n"
     ]
    }
   ],
   "source": [
    "# Check for errors and print response\n",
    "if response.status_code == 200:\n",
    "    result = response.json()\n",
    "    print(result[\"choices\"][0][\"message\"][\"content\"])\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20500f13-5dc6-41ce-b92a-c5bb1f09b727",
   "metadata": {},
   "source": [
    "### definition for llm in langchain (doesnt have tool integration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1183936f-6de4-42ae-8b63-0f420b195859",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.language_models import LLM\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class OpenRouterLLM(LLM):\n",
    "    openrouter_api_key: str\n",
    "    model: str\n",
    "    provider_order: list\n",
    "\n",
    "    def _call(self, prompt: str, stop=None) -> str:\n",
    "        \"\"\"Calls OpenRouter API and returns a response.\"\"\"\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.openrouter_api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "            \"provider\": {\"order\": self.provider_order},  # Provider preference\n",
    "        }\n",
    "\n",
    "        response = requests.post(\n",
    "            url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
    "            headers=headers,\n",
    "            data=json.dumps(data)\n",
    "        )\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            return result[\"choices\"][0][\"message\"][\"content\"]\n",
    "        else:\n",
    "            raise ValueError(f\"Error {response.status_code}: {response.text}\")\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self):\n",
    "        \"\"\"Defines unique parameters for this LLM.\"\"\"\n",
    "        return {\"model\": self.model, \"provider_order\": self.provider_order}\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self):\n",
    "        return \"custom_openrouter\"\n",
    "\n",
    "\n",
    "\n",
    "# Define LLM with provider preference\n",
    "llm = OpenRouterLLM(\n",
    "    openrouter_api_key=api_key,\n",
    "    model=\"deepseek/deepseek-r1-distill-llama-70b:free\",\n",
    "    #model=\"qwen/qwen-turbo\",\n",
    "    #model=\"qwen/qwen-vl-plus:free\",\n",
    "    #model=\"qwen/qwen-turbo\",    \n",
    "    provider_order=[\"Alibaba\", \"Together\", \"Targon\"]  # Provider preference\n",
    ")        \n",
    "\n",
    "LLM_TYPE=\"custom\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18abb1f0-37d9-412d-ac00-73d34e63bb24",
   "metadata": {},
   "source": [
    "### llm definition using openai config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a873c32e-d605-4181-81e9-193ba8af1ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(\n",
    "        temperature=0.7,\n",
    "        model=\"qwen/qwen-turbo\", \n",
    "        openai_api_key=api_key,\n",
    "        openai_api_base=\"https://openrouter.ai/api/v1\",\n",
    "        #provider_order=['DeepInfra','Together'],\n",
    "        #headers={\"HTTP-Referer\": constants.OPENROUTER_REFERRER},    \n",
    "    )\n",
    "\n",
    "LLM_TYPE=\"openai_type\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a1ced32-da7a-41cf-9fee-5e04fc06add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#local_model = \"llama3.2\"\n",
    "#llm = ChatOllama(model=local_model)\n",
    "#LLM_TYPE=\"ollama\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7faec16-4d6c-4b52-95d9-fe57f350296f",
   "metadata": {},
   "source": [
    "## METHOD 1: FAST REPLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d8c6d-aea6-404c-8359-6257cd766c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "Prompt: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "chunks_answer = []\n",
    "for chunk in chain.stream({\"question\":query,\"context\":found_docs}):\n",
    "    print(chunk, end='')\n",
    "    chunks_answer.append(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc45d7f-75e0-433d-8395-907172293b6b",
   "metadata": {},
   "source": [
    "## METHOD 2 : NO TOOL OR MEMORY , BUT SOURCES (compatable with all llms including qwen/qwen-vl-plus:free)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4298d8ba-ebfe-4c5f-a69a-962b1cb66505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/apps/cache/python-envs/ML/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = qdrant.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "#use this generate function in case of openai format or ollama.\n",
    "def generate_orig(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "##use this generate function in case of openrouter from scratch (llm in langhain)\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    \n",
    "    # Ensure the correct format for messages\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "\n",
    "    # OpenRouterLLM returns a string, so no `.content` attribute\n",
    "    response_text = llm.invoke(messages)  # This is already a string\n",
    "\n",
    "    return {\"answer\": response_text}  # Directly return the string\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72debefc-af63-4b37-8cd4-48aa5e55616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile application and test\n",
    "\n",
    "if LLM_TYPE == \"ollama\" or LLM_TYPE == \"openai_type\":\n",
    "    graph_builder = StateGraph(State).add_sequence([retrieve, generate_orig])\n",
    "\n",
    "if LLM_TYPE == \"custom\":\n",
    "    graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c756feac-b99d-47af-a90c-494c177369f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df35bfe3-d81d-467c-ba25-3263e9f65a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(metadata={'source': 'docs/todi2021.pdf', 'file_path': 'docs/todi2021.pdf', 'page': 7, 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'Adapting User Interfaces with Model-based Reinforcement Learning', 'author': 'Kashyap Todi', 'subject': '-  Human-centered computing  ->  Interactive systems and tools.', 'keywords': 'Adaptive User Interfaces; Reinforcement Learning; Predictive Models; Monte Carlo Tree Search', 'creator': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2018/02/06 v6.86b Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.19', 'creationDate': 'D:20210314174636Z', 'modDate': \"D:20210318182844+02'00'\", 'trapped': '', '_id': 'cad0e432b27849789d833f9fd19fc6cb', '_collection_name': 'my_documents'}, page_content='then passed to a fully connected layer. Finally, the click history \\nis passed to an LSTM layer, which models sequential data and is \\nDesign Data\\nCurrent\\nMatrix\\nAdapted\\nMatrix\\nUser Data`\\nLSTM\\nDropout\\nFully\\nConnected\\nDifference\\nLSTM\\nDropout\\nFully\\nConnected\\nCurrent\\nFrequency\\nEstimated\\nFrequency\\nConcatenator\\nEmbedding\\nFlatten\\nDropout\\nFully\\nConnected\\nAdapted\\nMenu\\nDropout\\nFully\\nConnected\\nFully\\nConnected\\nReward\\nSerial\\nDropout\\nFully\\nConnected\\nFully\\nConnected\\nReward\\nSerial\\nDropout\\nFully\\nConnected\\nFully\\nConnected\\nReward\\nSerial\\nDifference\\nFigure 6: Our value network architecture takes menu design \\nand user features as input to provide individual reward pre\\xad\\ndictions.'), Document(metadata={'source': 'docs/todi2021.pdf', 'file_path': 'docs/todi2021.pdf', 'page': 8, 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'Adapting User Interfaces with Model-based Reinforcement Learning', 'author': 'Kashyap Todi', 'subject': '-  Human-centered computing  ->  Interactive systems and tools.', 'keywords': 'Adaptive User Interfaces; Reinforcement Learning; Predictive Models; Monte Carlo Tree Search', 'creator': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2018/02/06 v6.86b Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.19', 'creationDate': 'D:20210314174636Z', 'modDate': \"D:20210318182844+02'00'\", 'trapped': '', '_id': '662096a58cd04835bb50d0569583a8b3', '_collection_name': 'my_documents'}, page_content='wards: model-based simulations and value neural network predictions. \\nWith model-based simulations, predictive models are used during \\nroll-outs to estimate rewards for each state. With value network \\npredictions, our pre-trained network models are used to predict \\nvalue estimates for each state. \\n6.1.2 Implementation. Our MCTS-based planning algorithm, and \\nthe predictive menu models, are implemented in Python 3.7. The \\nvalue neural network is implemented with TensorFlow 2.0.0. We \\nGloves: 0.6\\nCarrot: 0.16\\nPanda: 0.1\\nTable: 0.06\\nBeans: 0.06\\nBikini: 0.02\\n(Others: 0)\\nUser Interest\\n(Frequencies)\\nPredicted Average\\nSelection Time (s)\\n13.6s\\n7.2s\\nCurrent\\nAdapted\\nAdapt x 3\\nFigure 7: A sample result from the simulation study, for a 15\\xad\\nitem menu design. In 3 steps, the menu was adapted to better \\nsuit the given user’s interests (‘Gloves’ group moved to the \\ntop), and improved some grouping (‘Carrot’ with ‘Potato’) as \\nwell. \\nused a GNU/Linux server with an Intel Xeon Gold CPU @ 2.10 GHz \\n(64 bit processor) for simulations. The execution of the study was \\nautomated such that trials were conducted sequentially, to avoid \\nvariations in computational resource usage. \\nDuring each trial, a combination of {menu size × user history × \\nmenu design × objective function × reward source} was selected, \\nand given as input to the system. In a constrained setting, the MCTS \\nalgorithm was allowed 400 iterations, and a shallow roll-out horizon'), Document(metadata={'source': 'docs/todi2021.pdf', 'file_path': 'docs/todi2021.pdf', 'page': 4, 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'Adapting User Interfaces with Model-based Reinforcement Learning', 'author': 'Kashyap Todi', 'subject': '-  Human-centered computing  ->  Interactive systems and tools.', 'keywords': 'Adaptive User Interfaces; Reinforcement Learning; Predictive Models; Monte Carlo Tree Search', 'creator': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2018/02/06 v6.86b Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.19', 'creationDate': 'D:20210314174636Z', 'modDate': \"D:20210318182844+02'00'\", 'trapped': '', '_id': 'dc11c6746b0246b4aad20597660a17c1', '_collection_name': 'my_documents'}, page_content='unexplored states. \\n...\\n...\\n...\\n...\\nConcatenation\\nInput Branches \\n(Model)\\nInput\\nDesign\\nFeatures\\nUser\\nFeatures\\nOutput Branches\\n(Model)\\nValue\\nPredictions\\nModel-based\\nTraining Data\\nFigure 3: Neural network architecture for obtaining value \\nestimates. Training data is generated using HCI models. \\nAs illustrated in Figure 3, we propose an m-headed n-tailed ar\\xad\\nchitecture that is trained end-to-end with backpropagation. Each of \\nthe m input parameters is treated as an independent model branch \\n(head) that is eventually concatenated and passed to n independent \\nmodel branches (tails). During ofine training, model-based data \\nis generated using MCTS roll-outs from randomly sampled initial \\nstates. Value estimates, along with state (design and user) informa\\xad\\ntion, are given as input samples to a deep neural network model. \\nElementary design and user features are parameterised with the \\nneural network model. This is then used to predict value estimates \\nfor any given state in an online setting. Note that we decouple \\nvalue estimations from objective functions (see above), leaving it \\nup to the adaptive system to decide how to use information from \\nmultiple models. The main advantages of using neural networks \\nfor estimation are that they have high learning capacity, and can \\nevaluate thousands of states in real-time without running expensive \\nsimulations. \\n4.3 Applications in Adaptive Interfaces \\nWith certain conditions we outline here, the approach we outline'), Document(metadata={'source': 'docs/todi2021.pdf', 'file_path': 'docs/todi2021.pdf', 'page': 9, 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'Adapting User Interfaces with Model-based Reinforcement Learning', 'author': 'Kashyap Todi', 'subject': '-  Human-centered computing  ->  Interactive systems and tools.', 'keywords': 'Adaptive User Interfaces; Reinforcement Learning; Predictive Models; Monte Carlo Tree Search', 'creator': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2018/02/06 v6.86b Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.19', 'creationDate': 'D:20210314174636Z', 'modDate': \"D:20210318182844+02'00'\", 'trapped': '', '_id': '6cb458a7f86c4b2387ea096bd7b38c94', '_collection_name': 'my_documents'}, page_content='CHI ’21, May 8–13, 2021, Yokohama, Japan \\nKashyap Todi, Gilles Bailly, Luis A. Leiva, and Anti Oulasvirta \\nSimulation\\nValue Network\\n4\\n6\\n8\\n10\\n0\\n10\\n20\\n30\\n40\\n50\\nCompletion Time in s (400 iterations)\\nSearch Depth (Horizon)\\nFigure 8: Computation time for planning adaptations via \\nmodel-based simulations vs. value network for varying \\nsearch depths. Our value network exhibited consistent per\\xad\\nformance for longer horizons. \\n6, 8, and 10 – depicting a range of planning horizons, from short \\nsequences to longer sequences. \\nFigure 8 illustrates computation time results for each depth level \\n(for 400 MCTS iterations). We observe that for depths ≤ 4, the value \\nnetwork does not provide much beneft. However, as search depth \\nincreases, while the computation time with our value network \\nremains constant (mean M = 7.77s, SD = 1.0), it drastically increases \\nwith simulations (from M = 7.9s, SD = 3.5s at depth 4 to M = 39.0s, \\nSD = 7.4 at depth 10). \\n6.2 Empirical Evaluation \\nThe primary goal of this evaluation is to test whether our planning \\napproach (henceforth MCTS) applied to linear menus improves \\nperformance in comparison with static menus (Static), and with \\nthe well-known frequency-based adaptive approach (Freqency) \\nas a baseline (e.g. as in [34]). In MCTS, the menu adapts after each \\nblock by planning adaptations; in Static, the menu does not adapt \\nover time; in Freqency, the menu adapts based on the frequency \\nof clicks on menu items. To this end, we conducted a lab study')]\n",
      "\n",
      "\n",
      "Answer: A value network is a neural network model that predicts reward values for different states, using design and user features as inputs. It is trained to provide efficient, real-time value estimates, avoiding the computational cost of simulations, and is particularly effective for planning in applications like adaptive menu systems.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\": \"What is value network?\"})\n",
    "\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08eaace3-b18a-46ba-aba1-ecbc6df4c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b653f4-0cf8-421c-90e8-872aa1a97eda",
   "metadata": {},
   "source": [
    "## METHOD 3 : WITH MEMORY AND SOURCES (needs tool support in model, works with ollama and other paid models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a057940-5fbf-416e-abd2-50f5e8bd22e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve(query: str):\n",
    "    \"\"\"Retrieve information related to a query.\"\"\"\n",
    "    retrieved_docs = qdrant.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\n\" f\"Content: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "507d2787-a439-4d97-b8be-50feb25a67dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "from langgraph.graph import END, MessagesState, StateGraph\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "import re\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "import json\n",
    "\n",
    "\n",
    "class State(MessagesState):\n",
    "    context: List[Document]\n",
    "\n",
    "def create_prompt(state: State):\n",
    "    \"\"\"Formats the chat history and retrieved context into a proper prompt.\"\"\"\n",
    "    history = [\n",
    "        message for message in state[\"messages\"] if isinstance(message, (AIMessage, HumanMessage))\n",
    "    ]\n",
    "    \n",
    "    docs_content = \"\".join(state.get(\"context\", []))  # Retrieved documents\n",
    "    system_message = SystemMessage(\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following retrieved context to provide answers. \"\n",
    "        \"If the answer isn't available in the context, say you don't know.\"\n",
    "        f\"\\n\\nContext:\\n{docs_content}\"\n",
    "    )\n",
    "\n",
    "    return [system_message] + history\n",
    "\n",
    "\n",
    "\n",
    "# Step 1: Generate an AIMessage that may include a tool-call to be sent.\n",
    "\n",
    "#use this generate function in case of openai format or ollama.\n",
    "def query_or_respond_orig(state: State):\n",
    "    \"\"\"Generate tool call for retrieval or respond.\"\"\"\n",
    "    llm_with_tools = llm.bind_tools([retrieve]) \n",
    "    response = llm_with_tools.invoke(state[\"messages\"]) \n",
    "    # MessagesState appends messages to state instead of overwriting\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "#use this generate function in case of openrouter from scratch (llm in langhain)\n",
    "def query_or_respond(state: State):\n",
    "    \"\"\"Determine whether to retrieve documents or respond directly.\"\"\"\n",
    "    user_message = state[\"messages\"][-1]  # Get the latest user message\n",
    "\n",
    "    retrieved_docs = retrieve.invoke({\"query\": user_message.content})\n",
    "\n",
    "    #print('retreived docs 1',retrieved_docs)\n",
    "    \n",
    "    state['context'] = retrieved_docs\n",
    "    # Proceed with processing\n",
    "    prompt = create_prompt({\"messages\": state[\"messages\"], \"context\": state[\"context\"]})\n",
    "    response_text = llm.invoke(prompt)\n",
    "    response = AIMessage(content=response_text)\n",
    "    \n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "# Step 2: Execute the retrieval.\n",
    "tools = ToolNode([retrieve])\n",
    "\n",
    "\n",
    "# Step 3: Generate a response using the retrieved content.\n",
    "#use this generate function in case of openai format or ollama.\n",
    "def generate_orig(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run\n",
    "    response = llm.invoke(prompt)\n",
    "    context = []\n",
    "    for tool_message in tool_messages:\n",
    "        context.extend(tool_message.artifact)\n",
    "    return {\"messages\": [response], \"context\": context}\n",
    "\n",
    "\n",
    "#use this generate function in case of openrouter from scratch (llm in langhain)\n",
    "def generate(state: MessagesState):\n",
    "    \"\"\"Generate answer.\"\"\"\n",
    "    # Get generated ToolMessages\n",
    "    print('executed')\n",
    "    recent_tool_messages = []\n",
    "    for message in reversed(state[\"messages\"]):\n",
    "        if message.type == \"tool\":\n",
    "            recent_tool_messages.append(message)\n",
    "        else:\n",
    "            break\n",
    "    tool_messages = recent_tool_messages[::-1]\n",
    "\n",
    "    # Format into prompt\n",
    "    docs_content = \"\\n\\n\".join(doc.content for doc in tool_messages)\n",
    "    system_message_content = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        f\"{docs_content}\"\n",
    "    )\n",
    "    conversation_messages = [\n",
    "        message\n",
    "        for message in state[\"messages\"]\n",
    "        if message.type in (\"human\", \"system\")\n",
    "        or (message.type == \"ai\" and not message.tool_calls)\n",
    "    ]\n",
    "    prompt = [SystemMessage(system_message_content)] + conversation_messages\n",
    "\n",
    "    # Run LLM\n",
    "    response_text = llm.invoke(prompt)  # OpenRouterLLM returns a string\n",
    "\n",
    "    # Convert response string into AIMessage (fixes the issue)\n",
    "    response = AIMessage(content=response_text)\n",
    "\n",
    "    # Extract context from tool messages\n",
    "    context = []\n",
    "    for tool_message in tool_messages:\n",
    "        context.extend(tool_message.artifact)\n",
    "\n",
    "    return {\"messages\": [response], \"context\": context}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e63dfbad-0adb-4764-b79d-1cbc15dd317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(MessagesState)\n",
    "\n",
    "\n",
    "if LLM_TYPE == \"ollama\" or LLM_TYPE == \"openai_type\":\n",
    "    graph_builder.add_node(query_or_respond_orig)\n",
    "    graph_builder.add_node(tools)\n",
    "    graph_builder.add_node(generate_orig)\n",
    "    \n",
    "    graph_builder.set_entry_point(\"query_or_respond_orig\")\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"query_or_respond_orig\",\n",
    "        tools_condition,\n",
    "        {END: END, \"tools\": \"tools\"},\n",
    "    )\n",
    "    graph_builder.add_edge(\"tools\", \"generate_orig\")\n",
    "    graph_builder.add_edge(\"generate_orig\", END)\n",
    "\n",
    "if LLM_TYPE == \"custom\":\n",
    "    graph_builder.add_node(query_or_respond)\n",
    "    graph_builder.add_node(tools)\n",
    "    graph_builder.add_node(generate)\n",
    "    \n",
    "    graph_builder.set_entry_point(\"query_or_respond\")\n",
    "    graph_builder.add_conditional_edges(\n",
    "        \"query_or_respond\",\n",
    "        tools_condition,\n",
    "        {END: END, \"tools\": \"tools\"},\n",
    "    )\n",
    "    graph_builder.add_edge(\"tools\", \"generate\")\n",
    "    graph_builder.add_edge(\"generate\", END)\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "385f235d-f9c6-46bf-b8e1-614c64a99bd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3cec5-1382-4006-9ca7-109c302086b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "tell me about mcts\n"
     ]
    }
   ],
   "source": [
    "input_message = \"tell me about mcts\"\n",
    "\n",
    "for step in graph.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": input_message}]},\n",
    "    config,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    step[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e7b5d41-56f7-45c1-8908-d78d66344c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = \"whats value network?\"\n",
    "\n",
    "response = graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": input_message}]},config,stream_mode=\"values\",)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c42622b-6de3-4b54-bc35-c030a868e597",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d52b2f-ddf5-4d87-9947-b58bc0cfb969",
   "metadata": {},
   "source": [
    "## METHOD 4: CONVERSATIONAL RAG CHAIN (supports all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3ff8d3f-6c2e-41dc-be17-62a95034d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "\n",
    "### Contextualize question ###\n",
    "contextualize_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")\n",
    "\n",
    "\n",
    "### Answer question ###\n",
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "### Statefully manage chat history ###\n",
    "store = {}\n",
    "\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd91c90a-c535-48d7-95b1-68a28cf09b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The system uses a neural network architecture to predict value estimates for adaptive interfaces, handling design and user features through concatenated branches. It leverages Monte Carlo Tree Search (MCTS) for training data generation and processes sequential user interactions using LSTM layers. This setup allows efficient, real-time predictions without costly simulations, supporting dynamic applications effectively."
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in conversational_rag_chain.stream({\"input\": \"tell me a bit more?\"}, config={\"configurable\": {\"session_id\": \"abc123\"}}):\n",
    "    chunks.append(chunk)\n",
    "    if 'answer' in chunk:\n",
    "        print(chunk['answer'], end='')\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6a2ab82-3e41-4e9e-bd30-608e69040eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: [Document(metadata={'source': 'docs/todi2021.pdf', 'file_path': 'docs/todi2021.pdf', 'page': 4, 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'Adapting User Interfaces with Model-based Reinforcement Learning', 'author': 'Kashyap Todi', 'subject': '-  Human-centered computing  ->  Interactive systems and tools.', 'keywords': 'Adaptive User Interfaces; Reinforcement Learning; Predictive Models; Monte Carlo Tree Search', 'creator': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2018/02/06 v6.86b Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.19', 'creationDate': 'D:20210314174636Z', 'modDate': \"D:20210318182844+02'00'\", 'trapped': '', '_id': 'dc11c6746b0246b4aad20597660a17c1', '_collection_name': 'my_documents'}, page_content='unexplored states. \\n...\\n...\\n...\\n...\\nConcatenation\\nInput Branches \\n(Model)\\nInput\\nDesign\\nFeatures\\nUser\\nFeatures\\nOutput Branches\\n(Model)\\nValue\\nPredictions\\nModel-based\\nTraining Data\\nFigure 3: Neural network architecture for obtaining value \\nestimates. Training data is generated using HCI models. \\nAs illustrated in Figure 3, we propose an m-headed n-tailed ar\\xad\\nchitecture that is trained end-to-end with backpropagation. Each of \\nthe m input parameters is treated as an independent model branch \\n(head) that is eventually concatenated and passed to n independent \\nmodel branches (tails). During ofine training, model-based data \\nis generated using MCTS roll-outs from randomly sampled initial \\nstates. Value estimates, along with state (design and user) informa\\xad\\ntion, are given as input samples to a deep neural network model. \\nElementary design and user features are parameterised with the \\nneural network model. This is then used to predict value estimates \\nfor any given state in an online setting. Note that we decouple \\nvalue estimations from objective functions (see above), leaving it \\nup to the adaptive system to decide how to use information from \\nmultiple models. The main advantages of using neural networks \\nfor estimation are that they have high learning capacity, and can \\nevaluate thousands of states in real-time without running expensive \\nsimulations. \\n4.3 Applications in Adaptive Interfaces \\nWith certain conditions we outline here, the approach we outline'), Document(metadata={'source': 'docs/todi2021.pdf', 'file_path': 'docs/todi2021.pdf', 'page': 8, 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'Adapting User Interfaces with Model-based Reinforcement Learning', 'author': 'Kashyap Todi', 'subject': '-  Human-centered computing  ->  Interactive systems and tools.', 'keywords': 'Adaptive User Interfaces; Reinforcement Learning; Predictive Models; Monte Carlo Tree Search', 'creator': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2018/02/06 v6.86b Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.19', 'creationDate': 'D:20210314174636Z', 'modDate': \"D:20210318182844+02'00'\", 'trapped': '', '_id': '662096a58cd04835bb50d0569583a8b3', '_collection_name': 'my_documents'}, page_content='wards: model-based simulations and value neural network predictions. \\nWith model-based simulations, predictive models are used during \\nroll-outs to estimate rewards for each state. With value network \\npredictions, our pre-trained network models are used to predict \\nvalue estimates for each state. \\n6.1.2 Implementation. Our MCTS-based planning algorithm, and \\nthe predictive menu models, are implemented in Python 3.7. The \\nvalue neural network is implemented with TensorFlow 2.0.0. We \\nGloves: 0.6\\nCarrot: 0.16\\nPanda: 0.1\\nTable: 0.06\\nBeans: 0.06\\nBikini: 0.02\\n(Others: 0)\\nUser Interest\\n(Frequencies)\\nPredicted Average\\nSelection Time (s)\\n13.6s\\n7.2s\\nCurrent\\nAdapted\\nAdapt x 3\\nFigure 7: A sample result from the simulation study, for a 15\\xad\\nitem menu design. In 3 steps, the menu was adapted to better \\nsuit the given user’s interests (‘Gloves’ group moved to the \\ntop), and improved some grouping (‘Carrot’ with ‘Potato’) as \\nwell. \\nused a GNU/Linux server with an Intel Xeon Gold CPU @ 2.10 GHz \\n(64 bit processor) for simulations. The execution of the study was \\nautomated such that trials were conducted sequentially, to avoid \\nvariations in computational resource usage. \\nDuring each trial, a combination of {menu size × user history × \\nmenu design × objective function × reward source} was selected, \\nand given as input to the system. In a constrained setting, the MCTS \\nalgorithm was allowed 400 iterations, and a shallow roll-out horizon'), Document(metadata={'source': 'docs/todi2021.pdf', 'file_path': 'docs/todi2021.pdf', 'page': 4, 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'Adapting User Interfaces with Model-based Reinforcement Learning', 'author': 'Kashyap Todi', 'subject': '-  Human-centered computing  ->  Interactive systems and tools.', 'keywords': 'Adaptive User Interfaces; Reinforcement Learning; Predictive Models; Monte Carlo Tree Search', 'creator': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2018/02/06 v6.86b Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.19', 'creationDate': 'D:20210314174636Z', 'modDate': \"D:20210318182844+02'00'\", 'trapped': '', '_id': '8d46fd8470684d86b876bc20fa332141', '_collection_name': 'my_documents'}, page_content='costly changes. \\nWe support several ways for selecting adaptation that allows \\ncontrolling for the trade-of between risk and gain. Our approach \\nassumes that there are multiple predictive HCI models that bound \\nthe true behaviour, for example by ofering best-case and (realistic) \\nworst-case estimates. Alternatively, predictive models can be in\\xad\\ncluded to address multiple objectives, such as task completion time, \\ncognitive load, and disruption [25]. Combining value estimations \\nfrom all models, we can implement diferent objective functions, \\nsuch as: \\n(1) Average: The mean of rewards from each model gives total \\nreward r, thus accounting for varying user strategies. \\n(2) Optimistic: The system assumes optimal user strategy, and \\nselects the model that maximises rewards. \\n(3) Conservative: The system ensures that no user strategy is \\nharmed by selecting the lowest-possible reward, and penal\\xad\\nising negative rewards. \\n4.2 Value Estimation with Deep Neural \\nNetworks \\nTo address large problem sizes in online settings, where repeating \\na sufcient number of MCTS simulations to attain robust estimates \\nis infeasible, we develop a deep neural network architecture that \\ncan efciently provide predictions in real-time. In place of roll-\\nouts, where simulations can be costly for longer horizons, a pre\\xad\\ntrained value network is used to directly obtain value estimates for \\nunexplored states. \\n...\\n...\\n...\\n...\\nConcatenation\\nInput Branches \\n(Model)\\nInput\\nDesign\\nFeatures\\nUser\\nFeatures'), Document(metadata={'source': 'docs/todi2021.pdf', 'file_path': 'docs/todi2021.pdf', 'page': 1, 'total_pages': 13, 'format': 'PDF 1.7', 'title': 'Adapting User Interfaces with Model-based Reinforcement Learning', 'author': 'Kashyap Todi', 'subject': '-  Human-centered computing  ->  Interactive systems and tools.', 'keywords': 'Adaptive User Interfaces; Reinforcement Learning; Predictive Models; Monte Carlo Tree Search', 'creator': 'LaTeX with acmart 2020/11/15 v1.75 Typesetting articles for the Association for Computing Machinery and hyperref 2018/02/06 v6.86b Hypertext links for LaTeX', 'producer': 'pdfTeX-1.40.19', 'creationDate': 'D:20210314174636Z', 'modDate': \"D:20210318182844+02'00'\", 'trapped': '', '_id': '89dfa7f68d184695853357e2a1c43cb7', '_collection_name': 'my_documents'}, page_content='CHI ’21, May 8–13, 2021, Yokohama, Japan \\nKashyap Todi, Gilles Bailly, Luis A. Leiva, and Anti Oulasvirta \\n1 INTRODUCTION \\nAdaptive user interfaces can autonomously change the content, \\nlayout, or style of an interface to improve their ft with the user’s \\ncapabilities and interests. This paper looks at a foundational tech\\xad\\nnical problem of adaptive interfaces that lies at the intersection \\nof human–computer interaction and machine learning research: \\nHow to select adaptations? An adaptive system must decide what to \\nadapt, and when – or when not – to make changes. Diferent com\\xad\\nputational approaches to this problem have been studied, among \\nothers, rule-based systems, heuristics, bandits, Bayesian optimisa\\xad\\ntion, and supervised learning (see section 2). Although positive em\\xad\\npirical results have been obtained (e.g., [4, 13, 17, 46, 49, 51, 52, 54]), \\nknown approaches have been criticised for being unpredictable and \\nunreliable; they pick detrimental adaptations unacceptably often \\n[16, 22, 33, 36, 56]. \\nEstimation of utility is required for selecting an adaptation. Utility \\n– in this case – refers to the usefulness of an adaptation to the user, \\nor how it is perceived to beneft interaction when possible costs \\nare taken into account. Picking an adaptation can be considered \\na hypothesis on how useful it is for user. Unfortunately, utility \\nis very hard to estimate accurately – hard both at design time \\nas well as interactively from the kind of data these systems have')]\n",
      "\n",
      "\n",
      "Answer: The value network is a deep neural network that predicts value estimates for states in adaptive interfaces. It is trained using model-based data generated by Monte Carlo Tree Search (MCTS) roll-outs and processes design and user features through a multi-headed, multi-tailed architecture. This allows the system to make real-time predictions efficiently without relying on costly simulations.\n"
     ]
    }
   ],
   "source": [
    "result = conversational_rag_chain.invoke({\"input\": \"what is value network?\"}, config={\"configurable\": {\"session_id\": \"abc123\"}})\n",
    "\n",
    "print(f'Context: {result[\"context\"]}\\n\\n')\n",
    "print(f'Answer: {result[\"answer\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf0cd8-65ec-431c-9a39-b76ec2361055",
   "metadata": {},
   "source": [
    "## METHOD 5: AGENT EXECUTOR METHOD (support ollama and paid models, doesnt work with openrouter from scratch (custom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70f01dce-5fdc-4314-8fc4-2e2d073852c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "### Build retriever tool ###\n",
    "tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"blog_post_retriever\",\n",
    "    \"Searches and returns excerpts from the Autonomous Agents blog post.\",\n",
    ")\n",
    "tools = [tool]\n",
    "memory = MemorySaver()\n",
    "\n",
    "agent_executor = create_react_agent(llm, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "599efd40-2391-4a6c-9cd1-cff7f43b0cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content=\"It seems like your question is about a specific implementation, but it lacks context regarding what you're referring to. Could you please provide more details? For instance, are you asking about the implementation of a certain feature on a website, an algorithm, or perhaps a project related to autonomous agents? The more specifics you can give me, the better I can assist you with your query.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 76, 'prompt_tokens': 190, 'total_tokens': 266, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen/qwen-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-14d7e0af-ff71-4456-8ed2-34d5974f75a8-0', usage_metadata={'input_tokens': 190, 'output_tokens': 76, 'total_tokens': 266, 'input_token_details': {}, 'output_token_details': {}})]}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"How was it implemented?\"\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "for s in agent_executor.stream(\n",
    "    {\"messages\": [HumanMessage(content=query)]}, config=config\n",
    "):\n",
    "    print(s)\n",
    "    print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7262193-694c-423f-a60d-9e698654dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"whats value network?\"\n",
    "# Specify an ID for the thread\n",
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}\n",
    "\n",
    "response = agent_executor.invoke({\"messages\": [HumanMessage(content=query)]}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1c9bb7-b0ab-4a4a-acac-748ad3ee9cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486d2df4-0dd5-4e3f-b363-0d21ccef5d3e",
   "metadata": {},
   "source": [
    "# LLAMAINDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9eb84a7-3a7c-418b-b924-a3f4eb601099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"docs4\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416cf1b-286a-4273-807e-95889f02018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ollama\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.1\", request_timeout=300.0)\n",
    "#llm = Ollama(model=\"llama3\",base_url=\"http://192.168.1.232:11435\") #llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e949a-2658-48a1-93af-fb35b40da1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66caf2-3c75-4c82-b218-5fb94d6bfa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "#from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"mxbai-embed-large\",\n",
    "    #base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae78d59-2df9-4282-a05c-1b9ad26bebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set global parameters\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = ollama_embedding\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88c17d9-c2c0-47ec-bcca-09f3517f480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save to disc (Only execute this block if you have additional documents to be added to the database)\n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "client = qdrant_client.QdrantClient(path=\"./llamaindex_local_qdrant_pdf\")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"usability\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index_usability = VectorStoreIndex.from_documents( documents, storage_context=storage_context, embed_model=ollama_embedding, llm=llm, node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20), num_output = 512, context_window = 3900, show_progress=True)\n",
    "\n",
    "index_usability.storage_context.persist(persist_dir=\"./llamaindex_local_qdrant_pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be44569-042e-4614-a0c4-edcbd7d5bce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load from disc\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "client = qdrant_client.QdrantClient(path=\"./llamaindex_local_qdrant_pdf\")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"usability\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=\"./llamaindex_local_qdrant_pdf\")\n",
    "\n",
    "index_usability = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=ollama_embedding, llm=llm, node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20), num_output = 512, context_window = 3900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c0094a-dc27-499d-833d-8f9a393d8b3b",
   "metadata": {},
   "source": [
    "### Q AND A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac07b44-289c-4ef3-9ad0-773ee1423034",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Prompt\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"We have provided trusted context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this trusted and cientific information, please answer the question: {query_str}. Remember that the statements of the context are verfied and come from trusted sources.\\n\"\n",
    ")\n",
    "qa_template = Prompt(template)\n",
    "\n",
    "new_summary_tmpl_str = (\n",
    "    \"The original query is as follows: {query_str}\"\n",
    "    \"We have provided an existing answer: {existing_answer}\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more trusted context below. Remember that the statements of the context are verfied and come from trusted sources.\"\n",
    "    \"------------\"\n",
    "    \"{context_msg}\"\n",
    "    \"------------\"\n",
    "    \"Given the new trusted context, refine the original answer to better answer the query. If the context isn't useful, return the original answer. Remember that the statements of the new context are verfied and come from trusted sources.\"\n",
    "    \"Refined Answer: sure thing! \"\n",
    ")\n",
    "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d101a8a-8715-4205-a8b5-de7d68e31d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KeywordTableSimpleRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index_usability,\n",
    "    similarity_top_k=4, # Change this to lower value for higher performance\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer( ##try compact?\n",
    "    text_qa_template=qa_template,\n",
    "    streaming=True,\n",
    "    refine_template=new_summary_tmpl\n",
    ")\n",
    "query_engine3 = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[\n",
    "    #     SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    # ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da62ac98-4274-4642-9a2a-dd13444a0a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine3.query(\"Explain value network?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e599dd7-e685-4bef-ab9f-08ce88e08d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed5d4c-5594-4ee0-ad9e-2b4d61c7e642",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34325fab-4830-4380-8016-064e9c19e2b0",
   "metadata": {},
   "source": [
    "### chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae75b6-c564-4398-9bb5-997ac2bbb105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "Given a conversation (between Human and Assistant) and a follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<Follow Up Message>\n",
    "{question}\n",
    "\n",
    "<Standalone question>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# list of `ChatMessage` objects\n",
    "custom_chat_history = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Hello assistant, we are having a insightful discussion about Additive Manufacturing today.\",\n",
    "    ),\n",
    "    ChatMessage(role=MessageRole.ASSISTANT, content=\"Okay, sounds good.\"),\n",
    "]\n",
    "\n",
    "#query_engine = index_usability.as_query_engine()\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine3,\n",
    "    streaming=True,\n",
    "    #condense_question_prompt=custom_prompt,\n",
    "    #chat_history=custom_chat_history,\n",
    "    #verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0e5054-c7ad-4e84-b2ba-c690b71648e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.stream_chat(\"what all is composed in value network?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7219f28c-d956-48fe-b4b0-e3493cf7a06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5697b3d-6315-45b3-8915-9c1d86673dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724e57b-bb44-4214-ae9b-4b055adf6e44",
   "metadata": {},
   "source": [
    "### sources available only in Q an A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d8d287-7503-4dc6-9e37-116664f8eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fdb1a0-adae-4204-aae3-16b6f168f0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "if hasattr(response, 'metadata'):\n",
    "    document_info = str(response.metadata)\n",
    "    find = re.findall(r\"'page_label': '[^']*', 'file_name': '[^']*'\", document_info)\n",
    "\n",
    "    print('\\n'+'=' * 60+'\\n')\n",
    "    print('Context Information')\n",
    "    print(str(find))\n",
    "    print('\\n'+'=' * 60+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
