{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca745501-8e0d-4e01-a6c0-004059b536b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, TypedDict\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings, ChatOllama\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import START, StateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain.schema import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6c4d48-f0f4-4553-979a-05b752022a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.35it/s]\n"
     ]
    }
   ],
   "source": [
    "#note: pypdf also records title, page numeber etc required by chain-5\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "\n",
    "#loader = DirectoryLoader('/home/tom/Python/Tools/RAG and PDF/docs', glob=\"**/*.pdf\", loader_cls=UnstructuredPDFLoader, show_progress=True)\n",
    "loader = DirectoryLoader('docs', glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader, show_progress=True)\n",
    "repo_files = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4481e051-f795-491e-a0f2-f4544a9728de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files loaded: 13\n",
      "Number of documents : 63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Number of files loaded: {len(repo_files)}\")\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "documents = text_splitter.split_documents(documents=repo_files)\n",
    "print(f\"Number of documents : {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd9dbab-7246-4340-8b6d-d4c730576d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "#from langchain_community.vectorstores import Qdrant\n",
    "from langchain_qdrant import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5a15776-ab26-453f-a6e3-3732a092a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"nomic-embed-text\"\n",
    "embeddings = OllamaEmbeddings(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5367607f-4ba8-4ee5-9bbb-7a32599d1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore db\n",
    "\n",
    "qdrant = Qdrant.from_existing_collection(\n",
    " embeddings,\n",
    " path=\"langchain_local_qdrant_pdf\",\n",
    " collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a8afd26-2839-4b7d-a002-8bd63357616c",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = qdrant.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0910b170-f227-441e-b2dd-fea87983eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\"],\n",
    ")\n",
    "\n",
    "local_model = \"llama3.2\"\n",
    "llm = ChatOllama(model=local_model, temperature=0)\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa4091cd-fddf-44d0-a8d7-b66438eab5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[Document]\n",
    "\n",
    "def retrieve(state: GraphState):\n",
    "    \"\"\"\n",
    "    Retrieve documents from vectorstore\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state: GraphState):\n",
    "    \"\"\"\n",
    "    Generate answer using RAG on retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    generation = rag_chain.invoke({\"context\": formatted_docs, \"question\": question})\n",
    "    return {\"documents\": documents, \"question\": question, \"generation\": generation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c08855b2-a62d-47e4-8c3c-caba5e65ad1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x78d38059a6d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_edge(\"retrieve\", \"generate\")\n",
    "workflow.add_node(\"generate\", generate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c5d5eb8-39cf-4e96-9068-b7d8c0ce9005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add memory for checkpointing\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a085d0-84db-4aeb-817c-943f0f2fa9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Run a test question\n",
    "import uuid\n",
    "\n",
    "\n",
    "thread_id = uuid.uuid4()\n",
    "config = {\"configurable\": {\"thread_id\": thread_id}}\n",
    "\n",
    "\n",
    "state = {\"question\": \"What is agent memory?\"}\n",
    "output = app.stream(state, config)\n",
    "print(\"---FINAL OUTPUT STREAM---\")\n",
    "for step in output:\n",
    "    print(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920fce4f-e104-4e53-a62e-b37e9f3da1a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bc850-bfb7-4ad2-8d01-a164d7d3ccff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
