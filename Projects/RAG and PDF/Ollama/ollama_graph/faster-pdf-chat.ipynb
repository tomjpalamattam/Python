{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea43f19-1d50-487e-aadc-8efc64d99b78",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0140cebf-1845-41fa-8050-68a4c0f61cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: nomic-embed-text is broken in ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1f6b8c-a368-46ac-9f1b-33900a3ec326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "# source /home/tom/WD/.venv/bin/activate && /home/tom/WD/.venv/bin/pip install jupyterlab-lsp python-lsp-server llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-ollama unstructured langchain chromadb langchain-text-splitters google-cloud-vision google-cloud-storage deep_translator docx PyMuPDF llama-index-vector-stores-chroma torch torchvision torchtext langchainhub langchain-qdrant  langchain transformers accelerate sentence-transformers tensorflow langchain-community llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807e1f9c-d4d6-403b-8b98-d6711e595df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f049e4fa-0c28-4619-97c6-dd197dcfb346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.38it/s]\n"
     ]
    }
   ],
   "source": [
    "#note: pypdf also records title, page numeber etc required by chain-5\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "\n",
    "#loader = DirectoryLoader('/home/tom/Python/Tools/RAG and PDF/docs', glob=\"**/*.pdf\", loader_cls=UnstructuredPDFLoader, show_progress=True)\n",
    "loader = DirectoryLoader('docs', glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader, show_progress=True)\n",
    "repo_files = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71845e4f-50ae-44ce-b178-6947a46cc12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files loaded: 13\n",
      "Number of documents : 63\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Number of files loaded: {len(repo_files)}\")\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "documents = text_splitter.split_documents(documents=repo_files)\n",
    "print(f\"Number of documents : {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe9b371-6815-4ab4-8852-a7ff292c894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "#from langchain_community.vectorstores import Qdrant\n",
    "from langchain_qdrant import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb942e5f-c4aa-41b3-8629-ceaa5ff5164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mxbai-embed-large\"\n",
    "embeddings = OllamaEmbeddings(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53bd1a7c-35a1-42cb-aaa3-b110036ef071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index new docs to db\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    " documents,\n",
    " embeddings,\n",
    " path=\"langchain_local_qdrant_pdf_orig\",\n",
    " collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686781ac-c9aa-44cd-a6af-b0accd9016aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore db\n",
    "\n",
    "qdrant = Qdrant.from_existing_collection(\n",
    " embeddings,\n",
    " path=\"langchain_local_qdrant_pdf\",\n",
    " collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f66dd8-fa85-4a51-b5c0-93a09a8461ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(documents):\n",
    "    for doc in documents:\n",
    "        print(doc.metadata)\n",
    "        print(\" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \")\n",
    "        print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5949b456-d0f8-4f69-86f1-47c280f30234",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"what is value network?\"\n",
    "found_docs = qdrant.similarity_search(query)\n",
    "#pretty_print_docs(found_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d8335dc-200f-4e54-9546-61506c97af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "# LLM from Ollama\n",
    "local_model = \"llama3.2\"\n",
    "llm = ChatOllama(model=local_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63117ea9-a7da-4959-b0f9-2974292797eb",
   "metadata": {},
   "source": [
    "### Fast reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80d201a0-26ab-4848-b474-ee2b997e2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "Prompt: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "#response = chain.invoke({\"question\":query,\"context\":found_docs})\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d64263c-353d-4099-be06-750190fd8527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the International Standards Organization, usability refers to \"the extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency and satisfaction in a specified context of use.\" It includes aspects such as usefulness, effectiveness (ease of use), learnability, and attitude (likeability)."
     ]
    }
   ],
   "source": [
    "\n",
    "chunks_answer = []\n",
    "for chunk in chain.stream({\"question\":query,\"context\":found_docs}):\n",
    "    print(chunk, end='')\n",
    "    chunks_answer.append(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff7651-8063-4ebf-b1cb-dd0413bfe8d6",
   "metadata": {},
   "source": [
    "### Chain Retreival replay (Slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1ff9d24-7e9c-4770-9e83-588c86e4fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12fde5dc-d5cc-4260-81f9-a58ffb0d6e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/apps/cache/python-envs/ML/lib/python3.11/site-packages/langsmith/client.py:241: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    qdrant.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "Prompt: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f189b327-7a23-49bb-9e55-36456f39ac6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9388/3772450333.py:14: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n"
     ]
    }
   ],
   "source": [
    "# Define templates for prompts\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List, Tuple\n",
    "from langchain.schema import format_document\n",
    "\n",
    "#Initialte chat_history\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "# Create a memory instance\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\", memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# Define steps for the chain\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define templates for prompts\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        human = \"HumanMessage: \" + dialogue_turn[0]\n",
    "        ai = \"AIMessage: \" + dialogue_turn[1]\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],        \n",
    "#        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "\n",
    "# Create the final chain by combining the steps\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a2888f9-42db-4dd1-98f4-c7e04f0f2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with new memory\n",
    "\n",
    "# Define templates for prompts\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from typing import List, Tuple\n",
    "from langchain.schema import format_document\n",
    "\n",
    "#Initialte chat_history\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "# Create a memory instance\n",
    "memory = ConversationKGMemory(\n",
    "    llm = llm, return_messages=True, output_key=\"answer\", input_key=\"question\", memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# Define steps for the chain\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define templates for prompts\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        human = \"HumanMessage: \" + dialogue_turn[0]\n",
    "        ai = \"AIMessage: \" + dialogue_turn[1]\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],        \n",
    "#        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "\n",
    "# Create the final chain by combining the steps\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3421d-9265-46c7-acb2-be540547f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream chain 7\n",
    "\n",
    "input = \"\"\"\n",
    "whats the value network for?\n",
    "\"\"\"\n",
    "inputs = {\"question\": input, \"chat_history\": chat_history}\n",
    "\n",
    "\n",
    "\n",
    "chunks = []\n",
    "chunks_answer = []\n",
    "for chunk in final_chain.stream(inputs):\n",
    "    chunks.append(chunk)\n",
    "    if 'answer' in chunk:\n",
    "        print(chunk['answer'].content, end='')\n",
    "        chunks_answer.append(chunk['answer'].content)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "#Below code might not work\n",
    "\n",
    "\n",
    "# Save the conversation in memory\n",
    "#generated_answer = chunks['answer']\n",
    "\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=input),\n",
    "    AIMessage(content=chunks_answer),\n",
    "    #AIMessage(content=result[\"answer\"].content),\n",
    "])\n",
    "\n",
    "\n",
    "# Load memory to see the conversation history\n",
    "#memory.load_memory_variables({})\n",
    "\n",
    "#memory.save_context(inputs, {\"answer\": generated_answer.content})\n",
    "memory.save_context(inputs, {\"answer\": chunks_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92238b-7c2b-45f0-a56e-aa41c628eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def extract_source_and_page(chunks):\n",
    "#    for chunk in chunks:\n",
    "#        if 'docs' in chunk:\n",
    "#            for doc in chunk['docs']:\n",
    "#                source = doc.metadata.get('source', 'Unknown Source')\n",
    "#                page = doc.metadata.get('page', 'Unknown Page')\n",
    "#                print(f\"Source: {source}, Page: {page}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_source_and_page(chunks):\n",
    "    source_pages = {}\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if 'docs' in chunk:\n",
    "            for doc in chunk['docs']:\n",
    "                source = doc.metadata.get('source', 'Unknown Source')\n",
    "                page = doc.metadata.get('page', 'Unknown Page')\n",
    "                if source in source_pages:\n",
    "                    source_pages[source].append(page)\n",
    "                else:\n",
    "                    source_pages[source] = [page]\n",
    "\n",
    "    for source, pages in source_pages.items():\n",
    "        pages_str = \", \".join(map(str, pages))\n",
    "        print(f\"Source: {source}, Pages: {pages_str}\")\n",
    "\n",
    "extract_source_and_page(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c42f7-2635-4bbd-ab56-bb57d9044361",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266b3beb-204e-4cd8-9a2c-4331359d6878",
   "metadata": {},
   "source": [
    "# LLAMAINDEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27116c1d-4a29-4428-bd4d-ec8fe47c1a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "documents = SimpleDirectoryReader(\"docs4\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c71f2b79-1b36-423a-bed9-ae25f5d44085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import ollama\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.1\", request_timeout=300.0)\n",
    "#llm = Ollama(model=\"llama3\",base_url=\"http://192.168.1.232:11435\") #llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84f3f0c7-9ad4-427a-b776-b81aab885794",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    ServiceContext,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42d56e1b-772c-49c7-a3b6-32a73ca4229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "#from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "ollama_embedding = OllamaEmbedding(\n",
    "    model_name=\"mxbai-embed-large\",\n",
    "    #base_url=\"http://localhost:11434\",\n",
    "    ollama_additional_kwargs={\"mirostat\": 0},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6909724e-ce9e-4baa-a576-60ea42164d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set global parameters\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = ollama_embedding\n",
    "Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20)\n",
    "Settings.num_output = 512\n",
    "Settings.context_window = 3900\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5b6be3a-f3f9-4d45-a6ee-13c5fcdfeaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/WD/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|█████████████████████████████████████████████████████| 13/13 [00:00<00:00, 144.92it/s]\n",
      "Generating embeddings: 100%|██████████████████████████████████████████████| 51/51 [00:50<00:00,  1.01it/s]\n",
      "WARNING:root:Payload indexes have no effect in the local Qdrant. Please use server Qdrant if you need payload indexes.\n"
     ]
    }
   ],
   "source": [
    "#Save to disc (Only execute this block if you have additional documents to be added to the database)\n",
    "\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "client = qdrant_client.QdrantClient(path=\"./llamaindex_local_qdrant_pdf\")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"usability\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index_usability = VectorStoreIndex.from_documents( documents, storage_context=storage_context, embed_model=ollama_embedding, llm=llm, node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20), num_output = 512, context_window = 3900, show_progress=True)\n",
    "\n",
    "index_usability.storage_context.persist(persist_dir=\"./llamaindex_local_qdrant_pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f230e29-a0c0-4223-84e8-5a91fd01c5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load from disc\n",
    "import qdrant_client\n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "client = qdrant_client.QdrantClient(path=\"./llamaindex_local_qdrant_pdf\")\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client, collection_name=\"usability\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=\"./llamaindex_local_qdrant_pdf\")\n",
    "\n",
    "index_usability = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context, embed_model=ollama_embedding, llm=llm, node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=20), num_output = 512, context_window = 3900)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b435d9-3c81-4f4f-bdc3-4e5505da4278",
   "metadata": {},
   "source": [
    "### Q and A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d71d59b7-aad5-438c-91da-90c8c252b15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Prompt\n",
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = (\n",
    "    \"We have provided trusted context information below. \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"Given this trusted and cientific information, please answer the question: {query_str}. Remember that the statements of the context are verfied and come from trusted sources.\\n\"\n",
    ")\n",
    "qa_template = Prompt(template)\n",
    "\n",
    "new_summary_tmpl_str = (\n",
    "    \"The original query is as follows: {query_str}\"\n",
    "    \"We have provided an existing answer: {existing_answer}\"\n",
    "    \"We have the opportunity to refine the existing answer (only if needed) with some more trusted context below. Remember that the statements of the context are verfied and come from trusted sources.\"\n",
    "    \"------------\"\n",
    "    \"{context_msg}\"\n",
    "    \"------------\"\n",
    "    \"Given the new trusted context, refine the original answer to better answer the query. If the context isn't useful, return the original answer. Remember that the statements of the new context are verfied and come from trusted sources.\"\n",
    "    \"Refined Answer: sure thing! \"\n",
    ")\n",
    "new_summary_tmpl = PromptTemplate(new_summary_tmpl_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6adcc3c7-80e9-4992-80bb-0789e8fcce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.retrievers import BaseRetriever, VectorIndexRetriever, KeywordTableSimpleRetriever\n",
    "from llama_index.core import get_response_synthesizer\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index_usability,\n",
    "    similarity_top_k=4, # Change this to lower value for higher performance\n",
    ")\n",
    "\n",
    "response_synthesizer = get_response_synthesizer( ##try compact?\n",
    "    text_qa_template=qa_template,\n",
    "    streaming=True,\n",
    "    refine_template=new_summary_tmpl\n",
    ")\n",
    "query_engine3 = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    "    response_synthesizer=response_synthesizer,\n",
    "    # node_postprocessors=[\n",
    "    #     SimilarityPostprocessor(similarity_cutoff=0.7)\n",
    "    # ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f34b630-a821-474a-845b-21b3b40fc3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine3.query(\"Explain value network?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c19084ad-31f5-4d51-bf44-5b75012a6461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, a Value Network is a type of deep neural network architecture used to estimate values for unexplored states in a problem-solving setting. It is trained end-to-end with backpropagation and designed to provide predictions in real-time.\n",
      "\n",
      "In this specific study, the Value Network is used as an alternative to model-based simulations (MCTS) to estimate rewards or values for each state. The Value Network is pre-trained on data generated using MCTS roll-outs from randomly sampled initial states. It takes into account design features and user information to predict value estimates for any given state.\n",
      "\n",
      "The Value Network architecture consists of multiple input branches, which are treated as independent model branches that are eventually concatenated and passed to multiple output branches (tails). This m-headed n-tailed architecture is trained using a combination of model-based data generated from MCTS roll-outs and other training data.\n",
      "\n",
      "By utilizing the Value Network, the study aims to improve planning adaptations in linear menus by estimating values for unexplored states more efficiently than traditional MCTS methods."
     ]
    }
   ],
   "source": [
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84e5ae-c5cc-450d-8d8a-4278862f5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0409a44f-e772-4bff-82e2-b77772c6f337",
   "metadata": {},
   "source": [
    "### Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2763f710-31d2-45ed-bd27-d52feb6996d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "Given a conversation (between Human and Assistant) and a follow up message from Human, \\\n",
    "rewrite the message to be a standalone question that captures all relevant context \\\n",
    "from the conversation.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<Follow Up Message>\n",
    "{question}\n",
    "\n",
    "<Standalone question>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# list of `ChatMessage` objects\n",
    "custom_chat_history = [\n",
    "    ChatMessage(\n",
    "        role=MessageRole.USER,\n",
    "        content=\"Hello assistant, we are having a insightful discussion about Additive Manufacturing today.\",\n",
    "    ),\n",
    "    ChatMessage(role=MessageRole.ASSISTANT, content=\"Okay, sounds good.\"),\n",
    "]\n",
    "\n",
    "#query_engine = index_usability.as_query_engine()\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine3,\n",
    "    streaming=True,\n",
    "    #condense_question_prompt=custom_prompt,\n",
    "    #chat_history=custom_chat_history,\n",
    "    #verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8246e7fc-5161-4c60-b090-9bdfdfae667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.stream_chat(\"what all is composed in value network?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39107cf3-4e23-4575-ab4d-6fb6b857707c",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.print_response_stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e864934-5148-42c0-9bb3-477f3e13facf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>According to the provided text, the following components are composed in a Value Network:\n",
       "\n",
       "1. **Input Branches (Model) Input Design Features**: These refer to the design features used as input for the Value Network.\n",
       "2. **User Features**: These are the user-related features that serve as inputs for the Value Network.\n",
       "3. **Output Branches (Model) Value Predictions**: The output of the Value Network, which consists of predicted value estimates based on the input design and user features.\n",
       "\n",
       "In essence, a Value Network in this context takes in both design and user features as inputs and generates value predictions or estimates as outputs.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "display(Markdown(f\"<b>{response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c9701-df7d-4353-905b-932f9e697241",
   "metadata": {},
   "source": [
    "### sources available only in Q an A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdaa9141-d76d-4746-9c84-b4c2d3c99186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'afe201a8-0d02-4683-868c-2b6b44405f65': {'page_label': '10',\n",
       "  'file_name': 'todi2021.pdf',\n",
       "  'file_path': '/home/tom/WD/Tools/RAG and PDF/Ollama/test/docs/todi2021.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 5440821,\n",
       "  'creation_date': '2024-07-25',\n",
       "  'last_modified_date': '2023-11-09'},\n",
       " '3a000054-5a30-43d7-9e98-d472cfdd5024': {'page_label': '5',\n",
       "  'file_name': 'todi2021.pdf',\n",
       "  'file_path': '/home/tom/WD/Tools/RAG and PDF/Ollama/test/docs/todi2021.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 5440821,\n",
       "  'creation_date': '2024-07-25',\n",
       "  'last_modified_date': '2023-11-09'},\n",
       " 'a502f251-dcfa-463d-8f77-785e00f6ec9d': {'page_label': '12',\n",
       "  'file_name': 'todi2021.pdf',\n",
       "  'file_path': '/home/tom/WD/Tools/RAG and PDF/Ollama/test/docs/todi2021.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 5440821,\n",
       "  'creation_date': '2024-07-25',\n",
       "  'last_modified_date': '2023-11-09'},\n",
       " 'e9a4bb05-0595-4af1-85fb-eb0bceb4f058': {'page_label': '9',\n",
       "  'file_name': 'todi2021.pdf',\n",
       "  'file_path': '/home/tom/WD/Tools/RAG and PDF/Ollama/test/docs/todi2021.pdf',\n",
       "  'file_type': 'application/pdf',\n",
       "  'file_size': 5440821,\n",
       "  'creation_date': '2024-07-25',\n",
       "  'last_modified_date': '2023-11-09'}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a56ce72c-98e7-4574-9f80-5a796dfdf31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "\n",
      "Context Information\n",
      "[\"'page_label': '10', 'file_name': 'todi2021.pdf'\", \"'page_label': '5', 'file_name': 'todi2021.pdf'\", \"'page_label': '12', 'file_name': 'todi2021.pdf'\", \"'page_label': '9', 'file_name': 'todi2021.pdf'\"]\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "if hasattr(response, 'metadata'):\n",
    "    document_info = str(response.metadata)\n",
    "    find = re.findall(r\"'page_label': '[^']*', 'file_name': '[^']*'\", document_info)\n",
    "\n",
    "    print('\\n'+'=' * 60+'\\n')\n",
    "    print('Context Information')\n",
    "    print(str(find))\n",
    "    print('\\n'+'=' * 60+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb8d58-8326-4197-835c-fa269c6377e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
