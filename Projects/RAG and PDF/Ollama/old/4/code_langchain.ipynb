{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58359d56-dbb3-46bf-990e-1daddcd52e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchainhub\n",
    "!pip install langchain-qdrant\n",
    "!pip install langchain transformers accelerate sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d410316-b435-491d-be66-c6b91e10ff96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'chi21adaptive'...\n",
      "remote: Enumerating objects: 70, done.\u001b[K\n",
      "remote: Counting objects: 100% (70/70), done.\u001b[K\n",
      "remote: Compressing objects: 100% (52/52), done.\u001b[K\n",
      "remote: Total 70 (delta 17), reused 53 (delta 10), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (70/70), 370.94 KiB | 563.00 KiB/s, done.\n",
      "Resolving deltas: 100% (17/17), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/aalto-ui/chi21adaptive.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7fd2087-b002-454d-af7c-711be8f0eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "def convert_files_to_txt(src_dir, dst_dir):\n",
    "    # If the destination directory does not exist, create it.\n",
    "    if not os.path.exists(dst_dir):\n",
    "        os.makedirs(dst_dir)\n",
    "    \n",
    "    for root, dirs, files in os.walk(src_dir):\n",
    "        for file in files:\n",
    "            if not file.endswith('.jpg'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Get the relative path to preserve directory structure\n",
    "                rel_path = os.path.relpath(file_path, src_dir)\n",
    "                \n",
    "                # Create the same directory structure in the new directory\n",
    "                new_root = os.path.join(dst_dir, os.path.dirname(rel_path))\n",
    "                os.makedirs(new_root, exist_ok=True)\n",
    "                \n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = f.read()\n",
    "                except UnicodeDecodeError:\n",
    "                    try:\n",
    "                        with open(file_path, 'r', encoding='latin-1') as f:\n",
    "                            data = f.read()\n",
    "                    except UnicodeDecodeError:\n",
    "                        print(f\"Failed to decode the file: {file_path}\")\n",
    "                        continue\n",
    "                \n",
    "                # Create a new file path with .txt extension\n",
    "                new_file_path = os.path.join(new_root, file + '.txt')\n",
    "                with open(new_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(data)\n",
    "\n",
    "# Call the function with the source and destination directory paths\n",
    "convert_files_to_txt('chi21adaptive', 'converted_codebase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4beac09a-9089-4606-97e5-931995f01493",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files loaded: 0\n",
      "Number of documents : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader,TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "src_dir = \"converted_codebase\"\n",
    "loader = DirectoryLoader(src_dir, show_progress=True, loader_cls=TextLoader)\n",
    "repo_files = loader.load()\n",
    "print(f\"Number of files loaded: {len(repo_files)}\")\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "documents = text_splitter.split_documents(documents=repo_files)\n",
    "print(f\"Number of documents : {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba9a8502-6bb6-43ea-b0f4-81254102d74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    " old_path_with_txt_extension = doc.metadata[\"source\"]\n",
    " new_path_without_txt_extension = old_path_with_txt_extension.replace(\".txt\", \"\")\n",
    " doc.metadata.update({\"source\": new_path_without_txt_extension})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44b3148e-d7a3-43f9-87ab-3aaf18439301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "#from langchain_community.vectorstores import Qdrant\n",
    "from langchain_qdrant import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc7ec3dd-1dc3-4939-b1b5-bf2590e27518",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mxbai-embed-large\"\n",
    "embeddings = OllamaEmbeddings(model=model_name,\n",
    " show_progress=True,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9b45de4-dd6d-4532-9d5d-18ff5b319bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tom/WD/.venv/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/tom/WD/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "\n",
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "model_kwargs = {\"device\": \"cpu\"}\n",
    "encode_kwargs = {\"normalize_embeddings\":True}\n",
    "embeddings = HuggingFaceBgeEmbeddings(model_name=model_name,\n",
    " model_kwargs=model_kwargs,\n",
    " encode_kwargs=encode_kwargs,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df796b2e-f865-448a-a13f-72f0c15aa10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.78s/it]\n",
      "OllamaEmbeddings: 100%|███████████████████████████████████████████████████| 64/64 [00:46<00:00,  1.37it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████████████████████████████| 64/64 [00:54<00:00,  1.19it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████████████████████████████| 64/64 [00:53<00:00,  1.20it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████████████████████████████| 64/64 [00:50<00:00,  1.26it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████████████████████████████| 64/64 [00:50<00:00,  1.26it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████████████████████████████| 64/64 [00:53<00:00,  1.20it/s]\n",
      "OllamaEmbeddings: 100%|███████████████████████████████████████████████████| 56/56 [00:35<00:00,  1.57it/s]\n"
     ]
    }
   ],
   "source": [
    "#Index new docs to db\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    " documents,\n",
    " embeddings,\n",
    " path=\"local_qdrant\",\n",
    " collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f39598-0e21-4be9-b5a8-7b823f8ed2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore db\n",
    "\n",
    "qdrant = Qdrant.from_existing_collection(\n",
    " embeddings,\n",
    " path=\"local_qdrant\",\n",
    " collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8bdf038-4759-4f9d-bde3-a07e99e509ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(documents):\n",
    "    for doc in documents:\n",
    "        print(doc.metadata)\n",
    "        print(\" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \")\n",
    "        print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015a0f95-f455-426a-9383-169ef68608c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.10it/s]\n"
     ]
    }
   ],
   "source": [
    "query = \"whats policy network here?\"\n",
    "found_docs = qdrant.similarity_search(query)\n",
    "#pretty_print_docs(found_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88c901a8-3d39-4f9d-98cf-0226744f7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "# LLM from Ollama\n",
    "local_model = \"codellama\"\n",
    "llm = ChatOllama(model=local_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2481e537-bee0-497e-b2aa-fe500869c0f8",
   "metadata": {},
   "source": [
    "### Implementation 1 (directly from serach docs and not from retreiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ea11517-a1d1-48fa-8d67-1758b947009b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The policy network is a neural network architecture that is trained to predict the correct response for a given prompt or input. In this case, the policy network is being used to generate responses for the menu items in the context of a chatbot or virtual assistant. The policy network takes in the input (e.g. \"What's your favorite snack?\") and outputs a response (e.g. \"I don't have a personal preference, but some people like potato chips.\").\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "Prompt: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\":query,\"context\":found_docs})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ae152-cdf6-417c-9de7-1abf6d19d67e",
   "metadata": {},
   "source": [
    "### Implementation 2 (using retreiver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "74bdfefc-a2df-4363-875b-a96e903a2042",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cd0c279-b7b2-4c66-994f-0a0ac6121dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    qdrant.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "Prompt: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "814ef723-18a7-456f-bd6a-9b2f1c3e7cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define templates for prompts\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List, Tuple\n",
    "from langchain.schema import format_document\n",
    "\n",
    "#Initialte chat_history\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "# Create a memory instance\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\", memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# Define steps for the chain\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define templates for prompts\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        human = \"HumanMessage: \" + dialogue_turn[0]\n",
    "        ai = \"AIMessage: \" + dialogue_turn[1]\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],        \n",
    "#        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "\n",
    "# Create the final chain by combining the steps\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c74972b2-dbbc-433f-b8c3-492e36953041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:01<00:00,  1.98s/it]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00, 10.95it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00, 19.44it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.91it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.04it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.88it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00, 21.43it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  5.43it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00, 11.26it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.19it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.92it/s]\n",
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided code, it appears that the policy network is used to guide the user's search behavior in a menu-based system. The `UserOracle` class defines various methods related to user behavior, such as reading and recalling items from a menu.\n",
      "\n",
      "The `recall` method seems to be a key component of the policy network, as it takes into account the user's experience (activation points) and previous interactions to determine the probability of recalling an item. This suggests that the policy network is used to model the user's decision-making process when searching for items in the menu.\n",
      "\n",
      "In other words, the policy network aims to predict the likelihood of a user recalling an item from memory based on their past experiences and preferences. This can be seen as a form of cognitive modeling, where the network attempts to simulate how users make decisions and recall information."
     ]
    }
   ],
   "source": [
    "#stream chain 7\n",
    "\n",
    "input = \"\"\"\n",
    "whats the policy network for?\n",
    "\"\"\"\n",
    "inputs = {\"question\": input, \"chat_history\": chat_history}\n",
    "\n",
    "\n",
    "\n",
    "chunks = []\n",
    "chunks_answer = []\n",
    "for chunk in final_chain.stream(inputs):\n",
    "    chunks.append(chunk)\n",
    "    if 'answer' in chunk:\n",
    "        print(chunk['answer'].content, end='')\n",
    "        chunks_answer.append(chunk['answer'].content)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "#Below code might not work\n",
    "\n",
    "\n",
    "# Save the conversation in memory\n",
    "#generated_answer = chunks['answer']\n",
    "\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=input),\n",
    "    AIMessage(content=chunks_answer),\n",
    "    #AIMessage(content=result[\"answer\"].content),\n",
    "])\n",
    "\n",
    "\n",
    "# Load memory to see the conversation history\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "#memory.save_context(inputs, {\"answer\": generated_answer.content})\n",
    "memory.save_context(inputs, {\"answer\": chunks_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202854e6-6bf9-4172-a9f4-e2e213edd92e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
