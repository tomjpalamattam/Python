{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ea43f19-1d50-487e-aadc-8efc64d99b78",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0140cebf-1845-41fa-8050-68a4c0f61cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: nomic-embed-text is broken in ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1f6b8c-a368-46ac-9f1b-33900a3ec326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "# source /home/tom/WD/.venv/bin/activate && /home/tom/WD/.venv/bin/pip install jupyterlab-lsp python-lsp-server llama-index-core llama-index-readers-file llama-index-llms-ollama llama-index-embeddings-ollama unstructured langchain chromadb langchain-text-splitters google-cloud-vision google-cloud-storage deep_translator docx PyMuPDF llama-index-vector-stores-chroma torch torchvision torchtext langchainhub langchain-qdrant  langchain transformers accelerate sentence-transformers tensorflow langchain-community llama-index-vector-stores-qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "807e1f9c-d4d6-403b-8b98-d6711e595df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import UnstructuredPDFLoader\n",
    "from langchain_community.document_loaders import OnlinePDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f049e4fa-0c28-4619-97c6-dd197dcfb346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  7.52it/s]\n"
     ]
    }
   ],
   "source": [
    "#note: pypdf also records title, page numeber etc required by chain-5\n",
    "\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyMuPDFLoader\n",
    "\n",
    "#loader = DirectoryLoader('/home/tom/Python/Tools/RAG and PDF/docs', glob=\"**/*.pdf\", loader_cls=UnstructuredPDFLoader, show_progress=True)\n",
    "loader = DirectoryLoader('docs', glob=\"**/*.pdf\", loader_cls=PyMuPDFLoader, show_progress=True)\n",
    "repo_files = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71845e4f-50ae-44ce-b178-6947a46cc12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files loaded: 11\n",
      "Number of documents : 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"Number of files loaded: {len(repo_files)}\")\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "documents = text_splitter.split_documents(documents=repo_files)\n",
    "print(f\"Number of documents : {len(documents)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe9b371-6815-4ab4-8852-a7ff292c894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "#from langchain_community.vectorstores import Qdrant\n",
    "from langchain_qdrant import Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb942e5f-c4aa-41b3-8629-ceaa5ff5164e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mxbai-embed-large\"\n",
    "embeddings = OllamaEmbeddings(model=model_name,\n",
    " show_progress=True,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53bd1a7c-35a1-42cb-aaa3-b110036ef071",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.61s/it]\n",
      "OllamaEmbeddings: 100%|███████████████████████████████████████████████████| 63/63 [00:48<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "#Index new docs to db\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    " documents,\n",
    " embeddings,\n",
    " path=\"langchain_local_qdrant_pdf\",\n",
    " collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "686781ac-c9aa-44cd-a6af-b0accd9016aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Restore db\n",
    "\n",
    "qdrant = Qdrant.from_existing_collection(\n",
    " embeddings,\n",
    " path=\"langchain_local_qdrant_pdf\",\n",
    " collection_name=\"my_documents\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5f66dd8-fa85-4a51-b5c0-93a09a8461ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_docs(documents):\n",
    "    for doc in documents:\n",
    "        print(doc.metadata)\n",
    "        print(\" - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \")\n",
    "        print(doc.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5949b456-d0f8-4f69-86f1-47c280f30234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|█████████████████████████████████████████████████████| 1/1 [00:02<00:00,  2.41s/it]\n"
     ]
    }
   ],
   "source": [
    "query = \"question here?\"\n",
    "found_docs = qdrant.similarity_search(query)\n",
    "#pretty_print_docs(found_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d8335dc-200f-4e54-9546-61506c97af56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_core.prompts.chat import HumanMessagePromptTemplate\n",
    "\n",
    "# LLM from Ollama\n",
    "local_model = \"llama3.1\"\n",
    "llm = ChatOllama(model=local_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63117ea9-a7da-4959-b0f9-2974292797eb",
   "metadata": {},
   "source": [
    "### Fast reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80d201a0-26ab-4848-b474-ee2b997e2ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "Prompt: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "#response = chain.invoke({\"question\":query,\"context\":found_docs})\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64263c-353d-4099-be06-750190fd8527",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chunks_answer = []\n",
    "for chunk in chain.stream({\"question\":query,\"context\":found_docs}):\n",
    "    print(chunk, end='')\n",
    "    chunks_answer.append(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaff7651-8063-4ebf-b1cb-dd0413bfe8d6",
   "metadata": {},
   "source": [
    "### Chain Retreival replay (Slower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ff9d24-7e9c-4770-9e83-588c86e4fd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
    "    different versions of the given user question to retrieve relevant documents from\n",
    "    a vector database. By generating multiple perspectives on the user question, your\n",
    "    goal is to help the user overcome some of the limitations of the distance-based\n",
    "    similarity search. Provide these alternative questions separated by newlines.\n",
    "    Original question: {question}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "12fde5dc-d5cc-4260-81f9-a58ffb0d6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    qdrant.as_retriever(), \n",
    "    llm,\n",
    "    prompt=QUERY_PROMPT\n",
    ")\n",
    "\n",
    "# RAG prompt\n",
    "template = \"\"\"Answer the question based ONLY on the following context:\n",
    "{context}\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "Prompt: ChatPromptTemplate(input_variables=['context', 'question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f189b327-7a23-49bb-9e55-36456f39ac6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define templates for prompts\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from typing import List, Tuple\n",
    "from langchain.schema import format_document\n",
    "\n",
    "#Initialte chat_history\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "\n",
    "# Create a memory instance\n",
    "memory = ConversationBufferMemory(\n",
    "    return_messages=True, output_key=\"answer\", input_key=\"question\", memory_key=\"chat_history\"\n",
    ")\n",
    "\n",
    "# Define steps for the chain\n",
    "loaded_memory = RunnablePassthrough.assign(\n",
    "    chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Define templates for prompts\n",
    "_template = \"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Follow Up Input: {question}\n",
    "Standalone question:\"\"\"\n",
    "CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(_template)\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "ANSWER_PROMPT = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "\n",
    "\n",
    "def _format_chat_history(chat_history: List[Tuple]) -> str:\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        human = \"HumanMessage: \" + dialogue_turn[0]\n",
    "        ai = \"AIMessage: \" + dialogue_turn[1]\n",
    "        buffer += \"\\n\" + \"\\n\".join([human, ai])\n",
    "    return buffer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _combine_documents(\n",
    "    docs, document_prompt=DEFAULT_DOCUMENT_PROMPT, document_separator=\"\\n\\n\"\n",
    "):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "standalone_question = {\n",
    "    \"standalone_question\": {\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],        \n",
    "#        \"chat_history\": lambda x: _format_chat_history(x[\"chat_history\"]),\n",
    "    }\n",
    "    | CONDENSE_QUESTION_PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser(),\n",
    "}\n",
    "\n",
    "retrieved_documents = {\n",
    "    \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "    \"question\": lambda x: x[\"standalone_question\"],\n",
    "}\n",
    "\n",
    "final_inputs = {\n",
    "    \"context\": lambda x: _combine_documents(x[\"docs\"]),\n",
    "    \"question\": itemgetter(\"question\"),\n",
    "}\n",
    "\n",
    "answer = {\n",
    "    \"answer\": final_inputs | ANSWER_PROMPT | llm,\n",
    "    \"docs\": itemgetter(\"docs\"),\n",
    "}\n",
    "\n",
    "# Create the final chain by combining the steps\n",
    "final_chain = loaded_memory | standalone_question | retrieved_documents | answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb3421d-9265-46c7-acb2-be540547f0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stream chain 7\n",
    "\n",
    "input = \"\"\"\n",
    "Question Here\n",
    "\"\"\"\n",
    "inputs = {\"question\": input, \"chat_history\": chat_history}\n",
    "\n",
    "\n",
    "\n",
    "chunks = []\n",
    "chunks_answer = []\n",
    "for chunk in final_chain.stream(inputs):\n",
    "    chunks.append(chunk)\n",
    "    if 'answer' in chunk:\n",
    "        print(chunk['answer'].content, end='')\n",
    "        chunks_answer.append(chunk['answer'].content)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "\n",
    "#Below code might not work\n",
    "\n",
    "\n",
    "# Save the conversation in memory\n",
    "#generated_answer = chunks['answer']\n",
    "\n",
    "from langchain.schema.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history.extend([\n",
    "    HumanMessage(content=input),\n",
    "    AIMessage(content=chunks_answer),\n",
    "    #AIMessage(content=result[\"answer\"].content),\n",
    "])\n",
    "\n",
    "\n",
    "# Load memory to see the conversation history\n",
    "memory.load_memory_variables({})\n",
    "\n",
    "#memory.save_context(inputs, {\"answer\": generated_answer.content})\n",
    "memory.save_context(inputs, {\"answer\": chunks_answer})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b92238b-7c2b-45f0-a56e-aa41c628eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def extract_source_and_page(chunks):\n",
    "#    for chunk in chunks:\n",
    "#        if 'docs' in chunk:\n",
    "#            for doc in chunk['docs']:\n",
    "#                source = doc.metadata.get('source', 'Unknown Source')\n",
    "#                page = doc.metadata.get('page', 'Unknown Page')\n",
    "#                print(f\"Source: {source}, Page: {page}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_source_and_page(chunks):\n",
    "    source_pages = {}\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if 'docs' in chunk:\n",
    "            for doc in chunk['docs']:\n",
    "                source = doc.metadata.get('source', 'Unknown Source')\n",
    "                page = doc.metadata.get('page', 'Unknown Page')\n",
    "                if source in source_pages:\n",
    "                    source_pages[source].append(page)\n",
    "                else:\n",
    "                    source_pages[source] = [page]\n",
    "\n",
    "    for source, pages in source_pages.items():\n",
    "        pages_str = \", \".join(map(str, pages))\n",
    "        print(f\"Source: {source}, Pages: {pages_str}\")\n",
    "\n",
    "extract_source_and_page(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7c42f7-2635-4bbd-ab56-bb57d9044361",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
